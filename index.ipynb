{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll use a train-test partition as well as a validation set to get better insights about how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. From there, you'll define and compile the model like before. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Apply early stopping criteria with a neural network \n",
    "- Apply L1, L2, and dropout regularization on a neural network  \n",
    "- Examine the effects of training with more data on a neural network  \n",
    "\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "Run the following cell to import some of the libraries and classes you'll need in this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in the file `'Bank_complaints.csv'`. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preview the dataset\n",
    "df = pd.read_csv(\"Bank_complaints.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools such as regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* Train - test split\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels \n",
    "\n",
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training neural networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your model's performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "- Generate a random sample of 10,000 observations using seed 123 for consistency of results. \n",
    "- Split this sample into `X` and `y` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the data\n",
    "df_sample = df.sample(10**4, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 29561 to 4555\n",
      "Data columns (total 2 columns):\n",
      "Product                         10000 non-null object\n",
      "Consumer complaint narrative    10000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 234.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Student loan                   1878\n",
       "Consumer Loan                  1589\n",
       "Credit card                    1564\n",
       "Bank account or service        1437\n",
       "Mortgage                       1372\n",
       "Credit reporting               1151\n",
       "Checking or savings account    1009\n",
       "Name: Product, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.Product.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "y = df_sample.Product\n",
    "X = df_sample['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "- Split the data into training and test sets \n",
    "- Assign 1500 obervations to the test set and use 42 as the seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set \n",
    "\n",
    "As mentioned in the previous lesson, it is good practice to set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test set can then be used to determine an unbiased perforance of the model. \n",
    "\n",
    "Run the cell below to further divide the training data into training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing before building a neural network model. \n",
    "\n",
    "- Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "- Transform the training, validate, and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "# Only keep the 2000 most common words \n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train_final)\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_matrix(X_train_final, mode='binary')\n",
    "X_val_tokens = tokenizer.texts_to_matrix(X_val, mode='binary')\n",
    "X_test_tokens = tokenizer.texts_to_matrix(X_test, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero. \n",
    "\n",
    "Transform the training, validate, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the product labels to numerical values\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final)\n",
    "\n",
    "y_train_lb = to_categorical(lb.transform(y_train_final))[:, :, 1]\n",
    "y_val_lb = to_categorical(lb.transform(y_val))[:, :, 1]\n",
    "y_test_lb = to_categorical(lb.transform(y_test))[:, :, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Baseline Model \n",
    "\n",
    "Rebuild a fully connected (Dense) layer network:  \n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions (since you are dealing with a multiclass problem, classifying the complaints into 7 classes) \n",
    "- Use a `'softmax'` activation function for the output layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a baseline neural network model using Keras\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "baseline_model = models.Sequential()\n",
    "baseline_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "baseline_model.add(layers.Dense(25, activation='relu'))\n",
    "baseline_model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "Compile this model with: \n",
    "\n",
    "- a stochastic gradient descent optimizer \n",
    "- `'categorical_crossentropy'` as the loss function \n",
    "- a focus on `'accuracy'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "baseline_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                100050    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 182       \n",
      "=================================================================\n",
      "Total params: 101,507\n",
      "Trainable params: 101,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "- Train the model for 150 epochs in mini-batches of 256 samples \n",
    "- Include the `validation_data` argument to ensure you keep track of the validation loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/150\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.9688 - acc: 0.1217 - val_loss: 1.9460 - val_acc: 0.1460\n",
      "Epoch 2/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9393 - acc: 0.1747 - val_loss: 1.9269 - val_acc: 0.1800\n",
      "Epoch 3/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9183 - acc: 0.2040 - val_loss: 1.9100 - val_acc: 0.2260\n",
      "Epoch 4/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8986 - acc: 0.2232 - val_loss: 1.8920 - val_acc: 0.2280\n",
      "Epoch 5/150\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.8776 - acc: 0.2401 - val_loss: 1.8717 - val_acc: 0.2440\n",
      "Epoch 6/150\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.8545 - acc: 0.2527 - val_loss: 1.8489 - val_acc: 0.2540\n",
      "Epoch 7/150\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.8286 - acc: 0.2700 - val_loss: 1.8233 - val_acc: 0.2740\n",
      "Epoch 8/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.7994 - acc: 0.3063 - val_loss: 1.7941 - val_acc: 0.2970\n",
      "Epoch 9/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7665 - acc: 0.3303 - val_loss: 1.7621 - val_acc: 0.3200\n",
      "Epoch 10/150\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.7299 - acc: 0.3661 - val_loss: 1.7255 - val_acc: 0.3480\n",
      "Epoch 11/150\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.6895 - acc: 0.4028 - val_loss: 1.6854 - val_acc: 0.3740\n",
      "Epoch 12/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.6461 - acc: 0.4332 - val_loss: 1.6417 - val_acc: 0.4080\n",
      "Epoch 13/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.5997 - acc: 0.4608 - val_loss: 1.5952 - val_acc: 0.4380\n",
      "Epoch 14/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5511 - acc: 0.4845 - val_loss: 1.5471 - val_acc: 0.4560\n",
      "Epoch 15/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.5010 - acc: 0.5055 - val_loss: 1.4986 - val_acc: 0.4920\n",
      "Epoch 16/150\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.4505 - acc: 0.5280 - val_loss: 1.4459 - val_acc: 0.5090\n",
      "Epoch 17/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.3998 - acc: 0.5467 - val_loss: 1.3963 - val_acc: 0.5240\n",
      "Epoch 18/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3502 - acc: 0.5663 - val_loss: 1.3482 - val_acc: 0.5450\n",
      "Epoch 19/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.3018 - acc: 0.5841 - val_loss: 1.3017 - val_acc: 0.5700\n",
      "Epoch 20/150\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2555 - acc: 0.6011 - val_loss: 1.2555 - val_acc: 0.5860\n",
      "Epoch 21/150\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2108 - acc: 0.6171 - val_loss: 1.2130 - val_acc: 0.6140\n",
      "Epoch 22/150\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1687 - acc: 0.6316 - val_loss: 1.1730 - val_acc: 0.6180\n",
      "Epoch 23/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1293 - acc: 0.6441 - val_loss: 1.1343 - val_acc: 0.6410\n",
      "Epoch 24/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0920 - acc: 0.6548 - val_loss: 1.0996 - val_acc: 0.6430\n",
      "Epoch 25/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0576 - acc: 0.6620 - val_loss: 1.0692 - val_acc: 0.6530\n",
      "Epoch 26/150\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0258 - acc: 0.6721 - val_loss: 1.0370 - val_acc: 0.6570\n",
      "Epoch 27/150\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9960 - acc: 0.6804 - val_loss: 1.0077 - val_acc: 0.6680\n",
      "Epoch 28/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9686 - acc: 0.6880 - val_loss: 0.9819 - val_acc: 0.6690\n",
      "Epoch 29/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9428 - acc: 0.6951 - val_loss: 0.9573 - val_acc: 0.6730\n",
      "Epoch 30/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9194 - acc: 0.7019 - val_loss: 0.9375 - val_acc: 0.6750\n",
      "Epoch 31/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8975 - acc: 0.7068 - val_loss: 0.9219 - val_acc: 0.6820\n",
      "Epoch 32/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8774 - acc: 0.7112 - val_loss: 0.8982 - val_acc: 0.6830\n",
      "Epoch 33/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8580 - acc: 0.7161 - val_loss: 0.8804 - val_acc: 0.6870\n",
      "Epoch 34/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8408 - acc: 0.7200 - val_loss: 0.8674 - val_acc: 0.6880\n",
      "Epoch 35/150\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8242 - acc: 0.7240 - val_loss: 0.8545 - val_acc: 0.6910\n",
      "Epoch 36/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8089 - acc: 0.7277 - val_loss: 0.8390 - val_acc: 0.6950\n",
      "Epoch 37/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7945 - acc: 0.7312 - val_loss: 0.8277 - val_acc: 0.6940\n",
      "Epoch 38/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7812 - acc: 0.7328 - val_loss: 0.8165 - val_acc: 0.7090\n",
      "Epoch 39/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7684 - acc: 0.7377 - val_loss: 0.8038 - val_acc: 0.7160\n",
      "Epoch 40/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7561 - acc: 0.7425 - val_loss: 0.7979 - val_acc: 0.7070\n",
      "Epoch 41/150\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7453 - acc: 0.7436 - val_loss: 0.7850 - val_acc: 0.7130\n",
      "Epoch 42/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7340 - acc: 0.7456 - val_loss: 0.7777 - val_acc: 0.7080\n",
      "Epoch 43/150\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7236 - acc: 0.7500 - val_loss: 0.7692 - val_acc: 0.7070\n",
      "Epoch 44/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7138 - acc: 0.7533 - val_loss: 0.7634 - val_acc: 0.7090\n",
      "Epoch 45/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7042 - acc: 0.7573 - val_loss: 0.7569 - val_acc: 0.7120\n",
      "Epoch 46/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.6956 - acc: 0.7597 - val_loss: 0.7444 - val_acc: 0.7190\n",
      "Epoch 47/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6869 - acc: 0.7619 - val_loss: 0.7424 - val_acc: 0.7160\n",
      "Epoch 48/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6787 - acc: 0.7640 - val_loss: 0.7384 - val_acc: 0.7130\n",
      "Epoch 49/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6709 - acc: 0.7664 - val_loss: 0.7303 - val_acc: 0.7150\n",
      "Epoch 50/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6629 - acc: 0.7668 - val_loss: 0.7244 - val_acc: 0.7130\n",
      "Epoch 51/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.6555 - acc: 0.7696 - val_loss: 0.7211 - val_acc: 0.7170\n",
      "Epoch 52/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.6483 - acc: 0.7736 - val_loss: 0.7148 - val_acc: 0.7210\n",
      "Epoch 53/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6417 - acc: 0.7749 - val_loss: 0.7113 - val_acc: 0.7220\n",
      "Epoch 54/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.6351 - acc: 0.7767 - val_loss: 0.7059 - val_acc: 0.7250\n",
      "Epoch 55/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.6283 - acc: 0.7764 - val_loss: 0.7014 - val_acc: 0.7250\n",
      "Epoch 56/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.6221 - acc: 0.7784 - val_loss: 0.6974 - val_acc: 0.7200\n",
      "Epoch 57/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6160 - acc: 0.7813 - val_loss: 0.6953 - val_acc: 0.7200\n",
      "Epoch 58/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6103 - acc: 0.7811 - val_loss: 0.6895 - val_acc: 0.7260\n",
      "Epoch 59/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6044 - acc: 0.7847 - val_loss: 0.6918 - val_acc: 0.7280\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5986 - acc: 0.7879 - val_loss: 0.6900 - val_acc: 0.7260\n",
      "Epoch 61/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.5937 - acc: 0.7879 - val_loss: 0.6826 - val_acc: 0.7260\n",
      "Epoch 62/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5881 - acc: 0.7915 - val_loss: 0.6794 - val_acc: 0.7340\n",
      "Epoch 63/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5827 - acc: 0.7915 - val_loss: 0.6762 - val_acc: 0.7330\n",
      "Epoch 64/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5776 - acc: 0.7939 - val_loss: 0.6741 - val_acc: 0.7320\n",
      "Epoch 65/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.5725 - acc: 0.7957 - val_loss: 0.6721 - val_acc: 0.7310\n",
      "Epoch 66/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5676 - acc: 0.7977 - val_loss: 0.6722 - val_acc: 0.7340\n",
      "Epoch 67/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5629 - acc: 0.8004 - val_loss: 0.6679 - val_acc: 0.7240\n",
      "Epoch 68/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5580 - acc: 0.8021 - val_loss: 0.6628 - val_acc: 0.7340\n",
      "Epoch 69/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5531 - acc: 0.8047 - val_loss: 0.6621 - val_acc: 0.7360\n",
      "Epoch 70/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5492 - acc: 0.8076 - val_loss: 0.6627 - val_acc: 0.7320\n",
      "Epoch 71/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5445 - acc: 0.8075 - val_loss: 0.6593 - val_acc: 0.7270\n",
      "Epoch 72/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5398 - acc: 0.8107 - val_loss: 0.6566 - val_acc: 0.7320\n",
      "Epoch 73/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5358 - acc: 0.8123 - val_loss: 0.6573 - val_acc: 0.7320\n",
      "Epoch 74/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5318 - acc: 0.8109 - val_loss: 0.6547 - val_acc: 0.7380\n",
      "Epoch 75/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5272 - acc: 0.8163 - val_loss: 0.6560 - val_acc: 0.7310\n",
      "Epoch 76/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5234 - acc: 0.8171 - val_loss: 0.6526 - val_acc: 0.7410\n",
      "Epoch 77/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5189 - acc: 0.8184 - val_loss: 0.6499 - val_acc: 0.7410\n",
      "Epoch 78/150\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.5148 - acc: 0.8173 - val_loss: 0.6539 - val_acc: 0.7380\n",
      "Epoch 79/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5106 - acc: 0.8227 - val_loss: 0.6559 - val_acc: 0.7340\n",
      "Epoch 80/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5071 - acc: 0.8200 - val_loss: 0.6467 - val_acc: 0.7420\n",
      "Epoch 81/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5036 - acc: 0.8248 - val_loss: 0.6478 - val_acc: 0.7410\n",
      "Epoch 82/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.4992 - acc: 0.8267 - val_loss: 0.6447 - val_acc: 0.7440\n",
      "Epoch 83/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.4953 - acc: 0.8275 - val_loss: 0.6464 - val_acc: 0.7420\n",
      "Epoch 84/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4919 - acc: 0.8281 - val_loss: 0.6438 - val_acc: 0.7450\n",
      "Epoch 85/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.4882 - acc: 0.8301 - val_loss: 0.6444 - val_acc: 0.7480\n",
      "Epoch 86/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.4842 - acc: 0.8319 - val_loss: 0.6414 - val_acc: 0.7430\n",
      "Epoch 87/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4811 - acc: 0.8321 - val_loss: 0.6455 - val_acc: 0.7440\n",
      "Epoch 88/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4775 - acc: 0.8344 - val_loss: 0.6495 - val_acc: 0.7430\n",
      "Epoch 89/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4744 - acc: 0.8324 - val_loss: 0.6394 - val_acc: 0.7410\n",
      "Epoch 90/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.4707 - acc: 0.8372 - val_loss: 0.6376 - val_acc: 0.7450\n",
      "Epoch 91/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.4672 - acc: 0.8367 - val_loss: 0.6388 - val_acc: 0.7500\n",
      "Epoch 92/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.4636 - acc: 0.8381 - val_loss: 0.6415 - val_acc: 0.7470\n",
      "Epoch 93/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4603 - acc: 0.8428 - val_loss: 0.6358 - val_acc: 0.7480\n",
      "Epoch 94/150\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.4569 - acc: 0.8408 - val_loss: 0.6374 - val_acc: 0.7470\n",
      "Epoch 95/150\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.4538 - acc: 0.8449 - val_loss: 0.6355 - val_acc: 0.7480\n",
      "Epoch 96/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.4503 - acc: 0.8429 - val_loss: 0.6392 - val_acc: 0.7470\n",
      "Epoch 97/150\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4472 - acc: 0.8472 - val_loss: 0.6403 - val_acc: 0.7490\n",
      "Epoch 98/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4438 - acc: 0.8465 - val_loss: 0.6349 - val_acc: 0.7440\n",
      "Epoch 99/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4406 - acc: 0.8492 - val_loss: 0.6328 - val_acc: 0.7450\n",
      "Epoch 100/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.4379 - acc: 0.8496 - val_loss: 0.6348 - val_acc: 0.7420\n",
      "Epoch 101/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.4343 - acc: 0.8536 - val_loss: 0.6337 - val_acc: 0.7470\n",
      "Epoch 102/150\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.4311 - acc: 0.8561 - val_loss: 0.6337 - val_acc: 0.7430\n",
      "Epoch 103/150\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4280 - acc: 0.8557 - val_loss: 0.6375 - val_acc: 0.7500\n",
      "Epoch 104/150\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.4251 - acc: 0.8564 - val_loss: 0.6377 - val_acc: 0.7440\n",
      "Epoch 105/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4220 - acc: 0.8571 - val_loss: 0.6337 - val_acc: 0.7470\n",
      "Epoch 106/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.4190 - acc: 0.8587 - val_loss: 0.6308 - val_acc: 0.7490\n",
      "Epoch 107/150\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4163 - acc: 0.8611 - val_loss: 0.6332 - val_acc: 0.7460\n",
      "Epoch 108/150\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.4133 - acc: 0.8620 - val_loss: 0.6313 - val_acc: 0.7480\n",
      "Epoch 109/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.4103 - acc: 0.8640 - val_loss: 0.6367 - val_acc: 0.7470\n",
      "Epoch 110/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.4075 - acc: 0.8651 - val_loss: 0.6332 - val_acc: 0.7510\n",
      "Epoch 111/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4045 - acc: 0.8669 - val_loss: 0.6359 - val_acc: 0.7470\n",
      "Epoch 112/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4015 - acc: 0.8688 - val_loss: 0.6346 - val_acc: 0.7470\n",
      "Epoch 113/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3987 - acc: 0.8684 - val_loss: 0.6330 - val_acc: 0.7430\n",
      "Epoch 114/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.3959 - acc: 0.8699 - val_loss: 0.6344 - val_acc: 0.7520\n",
      "Epoch 115/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.3929 - acc: 0.8720 - val_loss: 0.6332 - val_acc: 0.7480\n",
      "Epoch 116/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.3906 - acc: 0.8737 - val_loss: 0.6361 - val_acc: 0.7460\n",
      "Epoch 117/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3878 - acc: 0.8743 - val_loss: 0.6355 - val_acc: 0.7530\n",
      "Epoch 118/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3847 - acc: 0.8749 - val_loss: 0.6315 - val_acc: 0.7500\n",
      "Epoch 119/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3824 - acc: 0.8752 - val_loss: 0.6338 - val_acc: 0.7470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3794 - acc: 0.8780 - val_loss: 0.6368 - val_acc: 0.7500\n",
      "Epoch 121/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3768 - acc: 0.8791 - val_loss: 0.6408 - val_acc: 0.7420\n",
      "Epoch 122/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.3738 - acc: 0.8796 - val_loss: 0.6326 - val_acc: 0.7530\n",
      "Epoch 123/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.3713 - acc: 0.8787 - val_loss: 0.6354 - val_acc: 0.7540\n",
      "Epoch 124/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.3686 - acc: 0.8823 - val_loss: 0.6369 - val_acc: 0.7490\n",
      "Epoch 125/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.3665 - acc: 0.8832 - val_loss: 0.6382 - val_acc: 0.7510\n",
      "Epoch 126/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3636 - acc: 0.8840 - val_loss: 0.6391 - val_acc: 0.7440\n",
      "Epoch 127/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.3612 - acc: 0.8859 - val_loss: 0.6356 - val_acc: 0.7450\n",
      "Epoch 128/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3585 - acc: 0.8853 - val_loss: 0.6492 - val_acc: 0.7410\n",
      "Epoch 129/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.3565 - acc: 0.8863 - val_loss: 0.6376 - val_acc: 0.7540\n",
      "Epoch 130/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.3534 - acc: 0.8879 - val_loss: 0.6363 - val_acc: 0.7460\n",
      "Epoch 131/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3507 - acc: 0.8879 - val_loss: 0.6384 - val_acc: 0.7500\n",
      "Epoch 132/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.3484 - acc: 0.8892 - val_loss: 0.6368 - val_acc: 0.7510\n",
      "Epoch 133/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.3462 - acc: 0.8903 - val_loss: 0.6385 - val_acc: 0.7440\n",
      "Epoch 134/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3435 - acc: 0.8903 - val_loss: 0.6402 - val_acc: 0.7520\n",
      "Epoch 135/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.3417 - acc: 0.8909 - val_loss: 0.6456 - val_acc: 0.7440\n",
      "Epoch 136/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3389 - acc: 0.8920 - val_loss: 0.6433 - val_acc: 0.7480\n",
      "Epoch 137/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3368 - acc: 0.8929 - val_loss: 0.6368 - val_acc: 0.7510\n",
      "Epoch 138/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3342 - acc: 0.8949 - val_loss: 0.6377 - val_acc: 0.7490\n",
      "Epoch 139/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.3318 - acc: 0.8948 - val_loss: 0.6511 - val_acc: 0.7440\n",
      "Epoch 140/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3301 - acc: 0.8964 - val_loss: 0.6468 - val_acc: 0.7510\n",
      "Epoch 141/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3270 - acc: 0.8967 - val_loss: 0.6414 - val_acc: 0.7470\n",
      "Epoch 142/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.3249 - acc: 0.8964 - val_loss: 0.6436 - val_acc: 0.7610\n",
      "Epoch 143/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.3230 - acc: 0.8983 - val_loss: 0.6443 - val_acc: 0.7500\n",
      "Epoch 144/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.3206 - acc: 0.9000 - val_loss: 0.6465 - val_acc: 0.7490\n",
      "Epoch 145/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3181 - acc: 0.8999 - val_loss: 0.6441 - val_acc: 0.7480\n",
      "Epoch 146/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.3158 - acc: 0.9013 - val_loss: 0.6476 - val_acc: 0.7470\n",
      "Epoch 147/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.3140 - acc: 0.9015 - val_loss: 0.6445 - val_acc: 0.7500\n",
      "Epoch 148/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3119 - acc: 0.9021 - val_loss: 0.6479 - val_acc: 0.7530\n",
      "Epoch 149/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3095 - acc: 0.9037 - val_loss: 0.6462 - val_acc: 0.7540\n",
      "Epoch 150/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.3076 - acc: 0.9044 - val_loss: 0.6461 - val_acc: 0.7470\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "baseline_model_val = baseline_model.fit(X_train_tokens, \n",
    "                                        y_train_lb, \n",
    "                                        epochs=150, \n",
    "                                        batch_size=256, \n",
    "                                        validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "The attribute `.history` (stored as a dictionary) contains four entries now: one per metric that was being monitored during training and validation. Print the keys of this dictionary for confirmation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the history attribute and store the dictionary\n",
    "baseline_model_val_dict = baseline_model_val.history\n",
    "\n",
    "# Print the keys\n",
    "baseline_model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 67us/step\n",
      "----------\n",
      "Training Loss: 0.304 \n",
      "Training Accuracy: 0.905\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "results_train = baseline_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print('----------')\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 114us/step\n",
      "----------\n",
      "Test Loss: 0.604 \n",
      "Test Accuracy: 0.787\n"
     ]
    }
   ],
   "source": [
    "results_test = baseline_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print('----------')\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results \n",
    "\n",
    "Plot the loss versus the number of epochs. Be sure to include the training and the validation loss in the same plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss vs number of epochs with train and validation sets\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "loss_values = baseline_model_val_dict['loss']\n",
    "val_loss_values = baseline_model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "ax.plot(epochs, loss_values, label='Training loss')\n",
    "ax.plot(epochs, val_loss_values, label='Validation loss')\n",
    "\n",
    "ax.set_title('Training & validation loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a second plot comparing training and validation accuracy to the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy vs number of epochs with train and validation sets\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "acc_values = baseline_model_val_dict['acc'] \n",
    "val_acc_values = baseline_model_val_dict['val_acc']\n",
    "\n",
    "ax.plot(epochs, acc_values, label='Training acc')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc')\n",
    "ax.set_title('Training & validation accuracy')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice an interesting pattern here? Although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss don't necessarily do the same. After a certain point, validation accuracy keeps swinging, which means that you're probably **overfitting** the model to the training data when you train for many epochs past a certain dropoff point. Let's tackle this now. You will now specify an early stopping point when training your model. \n",
    "\n",
    "\n",
    "## Early Stopping\n",
    "\n",
    "Overfitting neural networks is something you **_want_** to avoid at all costs. However, it's not possible to know in advance how many *epochs* you need to train your model on, and running the model multiple times with varying number of *epochs* maybe helpful, but is a time-consuming process. \n",
    "\n",
    "We've defined a model with the same architecture as above. This time specify an early stopping point when training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model_2.add(layers.Dense(25, activation='relu'))\n",
    "model_2.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model_2.compile(optimizer='SGD', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import `EarlyStopping` and `ModelCheckpoint` from `keras.callbacks` \n",
    "- Define a list, `early_stopping`: \n",
    "  - Monitor `'val_loss'` and continue training for 10 epochs before stopping \n",
    "  - Save the best model while monitoring `'val_loss'` \n",
    " \n",
    "> If you need help, consult [documentation](https://keras.io/callbacks/).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EarlyStopping and ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = [EarlyStopping(monitor='val_loss', patience=10), \n",
    "                  ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train `model_2`. Make sure you set the `callbacks` argument to `early_stopping`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/150\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 1.9358 - acc: 0.1807 - val_loss: 1.9279 - val_acc: 0.1910\n",
      "Epoch 2/150\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.9207 - acc: 0.1993 - val_loss: 1.9143 - val_acc: 0.2150\n",
      "Epoch 3/150\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.9067 - acc: 0.2212 - val_loss: 1.9009 - val_acc: 0.2230\n",
      "Epoch 4/150\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.8925 - acc: 0.2297 - val_loss: 1.8874 - val_acc: 0.2290\n",
      "Epoch 5/150\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.8774 - acc: 0.2399 - val_loss: 1.8731 - val_acc: 0.2290\n",
      "Epoch 6/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.8604 - acc: 0.2475 - val_loss: 1.8568 - val_acc: 0.2390\n",
      "Epoch 7/150\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.8412 - acc: 0.2553 - val_loss: 1.8381 - val_acc: 0.2460\n",
      "Epoch 8/150\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.8194 - acc: 0.2679 - val_loss: 1.8164 - val_acc: 0.2530\n",
      "Epoch 9/150\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.7951 - acc: 0.2821 - val_loss: 1.7928 - val_acc: 0.2690\n",
      "Epoch 10/150\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.7683 - acc: 0.3017 - val_loss: 1.7656 - val_acc: 0.2880\n",
      "Epoch 11/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.7391 - acc: 0.3248 - val_loss: 1.7379 - val_acc: 0.2980\n",
      "Epoch 12/150\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.7076 - acc: 0.3445 - val_loss: 1.7041 - val_acc: 0.3250\n",
      "Epoch 13/150\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.6735 - acc: 0.3755 - val_loss: 1.6710 - val_acc: 0.3400\n",
      "Epoch 14/150\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.6374 - acc: 0.3955 - val_loss: 1.6333 - val_acc: 0.4010\n",
      "Epoch 15/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.5991 - acc: 0.4291 - val_loss: 1.5951 - val_acc: 0.4180\n",
      "Epoch 16/150\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.5586 - acc: 0.4584 - val_loss: 1.5565 - val_acc: 0.4350\n",
      "Epoch 17/150\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.5166 - acc: 0.4773 - val_loss: 1.5148 - val_acc: 0.4780\n",
      "Epoch 18/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4726 - acc: 0.5084 - val_loss: 1.4706 - val_acc: 0.5020\n",
      "Epoch 19/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.4273 - acc: 0.5373 - val_loss: 1.4271 - val_acc: 0.5200\n",
      "Epoch 20/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3807 - acc: 0.5647 - val_loss: 1.3820 - val_acc: 0.5380\n",
      "Epoch 21/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3340 - acc: 0.5852 - val_loss: 1.3353 - val_acc: 0.5590\n",
      "Epoch 22/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2875 - acc: 0.6109 - val_loss: 1.2919 - val_acc: 0.5710\n",
      "Epoch 23/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.2417 - acc: 0.6312 - val_loss: 1.2466 - val_acc: 0.5940\n",
      "Epoch 24/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1972 - acc: 0.6471 - val_loss: 1.2011 - val_acc: 0.6170\n",
      "Epoch 25/150\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.1544 - acc: 0.6617 - val_loss: 1.1582 - val_acc: 0.6430\n",
      "Epoch 26/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1138 - acc: 0.6715 - val_loss: 1.1204 - val_acc: 0.6470\n",
      "Epoch 27/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0754 - acc: 0.6803 - val_loss: 1.0860 - val_acc: 0.6620\n",
      "Epoch 28/150\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0393 - acc: 0.6897 - val_loss: 1.0514 - val_acc: 0.6650\n",
      "Epoch 29/150\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.0060 - acc: 0.6965 - val_loss: 1.0180 - val_acc: 0.6760\n",
      "Epoch 30/150\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9749 - acc: 0.7037 - val_loss: 0.9886 - val_acc: 0.6770\n",
      "Epoch 31/150\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9461 - acc: 0.7105 - val_loss: 0.9599 - val_acc: 0.6870\n",
      "Epoch 32/150\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9193 - acc: 0.7147 - val_loss: 0.9333 - val_acc: 0.6890\n",
      "Epoch 33/150\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8945 - acc: 0.7188 - val_loss: 0.9139 - val_acc: 0.6880\n",
      "Epoch 34/150\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8721 - acc: 0.7245 - val_loss: 0.8935 - val_acc: 0.6970\n",
      "Epoch 35/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8510 - acc: 0.7275 - val_loss: 0.8741 - val_acc: 0.6980\n",
      "Epoch 36/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8317 - acc: 0.7333 - val_loss: 0.8603 - val_acc: 0.7000\n",
      "Epoch 37/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8137 - acc: 0.7399 - val_loss: 0.8436 - val_acc: 0.7040\n",
      "Epoch 38/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.7974 - acc: 0.7416 - val_loss: 0.8257 - val_acc: 0.7050\n",
      "Epoch 39/150\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.7819 - acc: 0.7444 - val_loss: 0.8149 - val_acc: 0.7030\n",
      "Epoch 40/150\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7676 - acc: 0.7467 - val_loss: 0.8002 - val_acc: 0.7060\n",
      "Epoch 41/150\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7542 - acc: 0.7504 - val_loss: 0.7909 - val_acc: 0.7070\n",
      "Epoch 42/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7417 - acc: 0.7539 - val_loss: 0.7812 - val_acc: 0.7030\n",
      "Epoch 43/150\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7298 - acc: 0.7569 - val_loss: 0.7726 - val_acc: 0.7070\n",
      "Epoch 44/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7186 - acc: 0.7596 - val_loss: 0.7628 - val_acc: 0.7140\n",
      "Epoch 45/150\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7080 - acc: 0.7608 - val_loss: 0.7566 - val_acc: 0.7250\n",
      "Epoch 46/150\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.6988 - acc: 0.7640 - val_loss: 0.7494 - val_acc: 0.7200\n",
      "Epoch 47/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6889 - acc: 0.7665 - val_loss: 0.7418 - val_acc: 0.7210\n",
      "Epoch 48/150\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.6801 - acc: 0.7689 - val_loss: 0.7371 - val_acc: 0.7250\n",
      "Epoch 49/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6715 - acc: 0.7739 - val_loss: 0.7315 - val_acc: 0.7250\n",
      "Epoch 50/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6636 - acc: 0.7735 - val_loss: 0.7246 - val_acc: 0.7270\n",
      "Epoch 51/150\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.6555 - acc: 0.7760 - val_loss: 0.7244 - val_acc: 0.7170\n",
      "Epoch 52/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.6486 - acc: 0.7765 - val_loss: 0.7123 - val_acc: 0.7350\n",
      "Epoch 53/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6413 - acc: 0.7799 - val_loss: 0.7117 - val_acc: 0.7290\n",
      "Epoch 54/150\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.6343 - acc: 0.7808 - val_loss: 0.7046 - val_acc: 0.7300\n",
      "Epoch 55/150\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.6274 - acc: 0.7844 - val_loss: 0.7040 - val_acc: 0.7270\n",
      "Epoch 56/150\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.6211 - acc: 0.7861 - val_loss: 0.7013 - val_acc: 0.7240\n",
      "Epoch 57/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.6148 - acc: 0.7876 - val_loss: 0.6921 - val_acc: 0.7310\n",
      "Epoch 58/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.6090 - acc: 0.7900 - val_loss: 0.6908 - val_acc: 0.7320\n",
      "Epoch 59/150\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.6028 - acc: 0.7940 - val_loss: 0.6856 - val_acc: 0.7300\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.5971 - acc: 0.7931 - val_loss: 0.6854 - val_acc: 0.7330\n",
      "Epoch 61/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.5913 - acc: 0.7977 - val_loss: 0.6807 - val_acc: 0.7350\n",
      "Epoch 62/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5860 - acc: 0.7980 - val_loss: 0.6782 - val_acc: 0.7400\n",
      "Epoch 63/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5806 - acc: 0.7993 - val_loss: 0.6776 - val_acc: 0.7390\n",
      "Epoch 64/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5753 - acc: 0.8007 - val_loss: 0.6768 - val_acc: 0.7330\n",
      "Epoch 65/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5701 - acc: 0.8016 - val_loss: 0.6714 - val_acc: 0.7380\n",
      "Epoch 66/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5651 - acc: 0.8056 - val_loss: 0.6683 - val_acc: 0.7300\n",
      "Epoch 67/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5605 - acc: 0.8064 - val_loss: 0.6672 - val_acc: 0.7390\n",
      "Epoch 68/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5554 - acc: 0.8075 - val_loss: 0.6635 - val_acc: 0.7390\n",
      "Epoch 69/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5506 - acc: 0.8117 - val_loss: 0.6631 - val_acc: 0.7380\n",
      "Epoch 70/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5460 - acc: 0.8116 - val_loss: 0.6616 - val_acc: 0.7340\n",
      "Epoch 71/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5415 - acc: 0.8135 - val_loss: 0.6576 - val_acc: 0.7430\n",
      "Epoch 72/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5371 - acc: 0.8187 - val_loss: 0.6569 - val_acc: 0.7370\n",
      "Epoch 73/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5318 - acc: 0.8224 - val_loss: 0.6623 - val_acc: 0.7430\n",
      "Epoch 74/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5285 - acc: 0.8199 - val_loss: 0.6555 - val_acc: 0.7420\n",
      "Epoch 75/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5239 - acc: 0.8233 - val_loss: 0.6546 - val_acc: 0.7470\n",
      "Epoch 76/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5194 - acc: 0.8241 - val_loss: 0.6511 - val_acc: 0.7380\n",
      "Epoch 77/150\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.5149 - acc: 0.8245 - val_loss: 0.6517 - val_acc: 0.7420\n",
      "Epoch 78/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5116 - acc: 0.8256 - val_loss: 0.6471 - val_acc: 0.7390\n",
      "Epoch 79/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.5069 - acc: 0.8289 - val_loss: 0.6561 - val_acc: 0.7380\n",
      "Epoch 80/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5030 - acc: 0.8292 - val_loss: 0.6518 - val_acc: 0.7370\n",
      "Epoch 81/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4991 - acc: 0.8312 - val_loss: 0.6433 - val_acc: 0.7440\n",
      "Epoch 82/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4951 - acc: 0.8337 - val_loss: 0.6481 - val_acc: 0.7450\n",
      "Epoch 83/150\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.4910 - acc: 0.8345 - val_loss: 0.6435 - val_acc: 0.7420\n",
      "Epoch 84/150\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.4873 - acc: 0.8371 - val_loss: 0.6435 - val_acc: 0.7440\n",
      "Epoch 85/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4832 - acc: 0.8395 - val_loss: 0.6417 - val_acc: 0.7440\n",
      "Epoch 86/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.4793 - acc: 0.8401 - val_loss: 0.6379 - val_acc: 0.7430\n",
      "Epoch 87/150\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.4764 - acc: 0.8417 - val_loss: 0.6464 - val_acc: 0.7400\n",
      "Epoch 88/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.4722 - acc: 0.8400 - val_loss: 0.6396 - val_acc: 0.7480\n",
      "Epoch 89/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.4688 - acc: 0.8453 - val_loss: 0.6374 - val_acc: 0.7450\n",
      "Epoch 90/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4651 - acc: 0.8444 - val_loss: 0.6372 - val_acc: 0.7500\n",
      "Epoch 91/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.4621 - acc: 0.8463 - val_loss: 0.6401 - val_acc: 0.7430\n",
      "Epoch 92/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.4584 - acc: 0.8471 - val_loss: 0.6375 - val_acc: 0.7470\n",
      "Epoch 93/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4545 - acc: 0.8504 - val_loss: 0.6365 - val_acc: 0.7480\n",
      "Epoch 94/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4513 - acc: 0.8515 - val_loss: 0.6381 - val_acc: 0.7510\n",
      "Epoch 95/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4481 - acc: 0.8508 - val_loss: 0.6334 - val_acc: 0.7450\n",
      "Epoch 96/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.4445 - acc: 0.8549 - val_loss: 0.6328 - val_acc: 0.7500\n",
      "Epoch 97/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4412 - acc: 0.8541 - val_loss: 0.6386 - val_acc: 0.7490\n",
      "Epoch 98/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.4382 - acc: 0.8564 - val_loss: 0.6379 - val_acc: 0.7440\n",
      "Epoch 99/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4351 - acc: 0.8564 - val_loss: 0.6336 - val_acc: 0.7510\n",
      "Epoch 100/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4318 - acc: 0.8592 - val_loss: 0.6353 - val_acc: 0.7490\n",
      "Epoch 101/150\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.4284 - acc: 0.8603 - val_loss: 0.6318 - val_acc: 0.7520\n",
      "Epoch 102/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.4253 - acc: 0.8616 - val_loss: 0.6333 - val_acc: 0.7510\n",
      "Epoch 103/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4221 - acc: 0.8639 - val_loss: 0.6336 - val_acc: 0.7520\n",
      "Epoch 104/150\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.4189 - acc: 0.8636 - val_loss: 0.6318 - val_acc: 0.7520\n",
      "Epoch 105/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4158 - acc: 0.8693 - val_loss: 0.6317 - val_acc: 0.7500\n",
      "Epoch 106/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.4128 - acc: 0.8680 - val_loss: 0.6312 - val_acc: 0.7530\n",
      "Epoch 107/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.4098 - acc: 0.8693 - val_loss: 0.6306 - val_acc: 0.7500\n",
      "Epoch 108/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4070 - acc: 0.8687 - val_loss: 0.6319 - val_acc: 0.7520\n",
      "Epoch 109/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4036 - acc: 0.8713 - val_loss: 0.6293 - val_acc: 0.7560\n",
      "Epoch 110/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.4012 - acc: 0.8732 - val_loss: 0.6329 - val_acc: 0.7530\n",
      "Epoch 111/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3981 - acc: 0.8743 - val_loss: 0.6330 - val_acc: 0.7520\n",
      "Epoch 112/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.3952 - acc: 0.8739 - val_loss: 0.6351 - val_acc: 0.7550\n",
      "Epoch 113/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.3923 - acc: 0.8753 - val_loss: 0.6315 - val_acc: 0.7540\n",
      "Epoch 114/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3892 - acc: 0.8752 - val_loss: 0.6292 - val_acc: 0.7510\n",
      "Epoch 115/150\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.3869 - acc: 0.8791 - val_loss: 0.6323 - val_acc: 0.7510\n",
      "Epoch 116/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.3839 - acc: 0.8752 - val_loss: 0.6283 - val_acc: 0.7500\n",
      "Epoch 117/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.3810 - acc: 0.8804 - val_loss: 0.6294 - val_acc: 0.7560\n",
      "Epoch 118/150\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.3785 - acc: 0.8800 - val_loss: 0.6321 - val_acc: 0.7480\n",
      "Epoch 119/150\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.3757 - acc: 0.8813 - val_loss: 0.6357 - val_acc: 0.7490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.3728 - acc: 0.8832 - val_loss: 0.6320 - val_acc: 0.7560\n",
      "Epoch 121/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3705 - acc: 0.8837 - val_loss: 0.6311 - val_acc: 0.7540\n",
      "Epoch 122/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3677 - acc: 0.8856 - val_loss: 0.6351 - val_acc: 0.7540\n",
      "Epoch 123/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.3654 - acc: 0.8856 - val_loss: 0.6323 - val_acc: 0.7580\n",
      "Epoch 124/150\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.3622 - acc: 0.8869 - val_loss: 0.6341 - val_acc: 0.7580\n",
      "Epoch 125/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3597 - acc: 0.8869 - val_loss: 0.6344 - val_acc: 0.7510\n",
      "Epoch 126/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.3572 - acc: 0.8893 - val_loss: 0.6406 - val_acc: 0.7560\n"
     ]
    }
   ],
   "source": [
    "# Train and fit the model\n",
    "model_2_val = model_2.fit(X_train_tokens, \n",
    "                          y_train_lb, \n",
    "                          epochs=150, \n",
    "                          callbacks=early_stopping, \n",
    "                          batch_size=256, \n",
    "                          validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best (saved) model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best (saved) model\n",
    "from keras.models import load_model\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this model to to calculate the training and test accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 122us/step\n",
      "Training Loss: 0.381 \n",
      "Training Accuracy: 0.879\n",
      "----------\n",
      "1500/1500 [==============================] - 0s 153us/step\n",
      "Test Loss: 0.598 \n",
      "Test Accuracy: 0.781\n"
     ]
    }
   ],
   "source": [
    "results_train = saved_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = saved_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! Did you notice that the model didn't train for all 150 epochs? You reduced your training time. \n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance. \n",
    "\n",
    "## L2 Regularization \n",
    "\n",
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=keras.regularizers.l2(lambda_coeff)` parameter to any model layer. The `lambda_coeff` parameter determines the strength of the regularization you wish to perform. \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L2 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regularizers\n",
    "\n",
    "random.seed(123)\n",
    "L2_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "\n",
    "\n",
    "# Add another hidden layer\n",
    "\n",
    "\n",
    "# Add an output layer\n",
    "L2_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L2_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model \n",
    "L2_model_val = L2_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training as well as the validation accuracy for both the L2 and the baseline models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 model details\n",
    "L2_model_dict = L2_model_val.history\n",
    "L2_acc_values = L2_model_dict['acc'] \n",
    "L2_val_acc_values = L2_model_dict['val_acc']\n",
    "\n",
    "# Baseline model\n",
    "baseline_model_acc = baseline_model_val_dict['acc'] \n",
    "baseline_model_val_acc = baseline_model_val_dict['val_acc']\n",
    "\n",
    "# Plot the accuracy for these models\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, L2_acc_values, label='Training acc L2')\n",
    "ax.plot(epochs, L2_val_acc_values, label='Validation acc L2')\n",
    "ax.plot(epochs, baseline_model_acc, label='Training acc')\n",
    "ax.plot(epochs, baseline_model_val_acc, label='Validation acc')\n",
    "ax.set_title('Training & validation accuracy L2 vs regular')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better.  \n",
    "\n",
    "\n",
    "## L1 Regularization\n",
    "\n",
    "Now have a look at L1 regularization. Will this work better? \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L1 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/150\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 16.0018 - acc: 0.1589 - val_loss: 15.5850 - val_acc: 0.2100\n",
      "Epoch 2/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 15.2336 - acc: 0.1891 - val_loss: 14.8389 - val_acc: 0.2220\n",
      "Epoch 3/150\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 14.4948 - acc: 0.2131 - val_loss: 14.1152 - val_acc: 0.2350\n",
      "Epoch 4/150\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 13.7772 - acc: 0.2387 - val_loss: 13.4080 - val_acc: 0.2610\n",
      "Epoch 5/150\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 13.0770 - acc: 0.2721 - val_loss: 12.7176 - val_acc: 0.2960\n",
      "Epoch 6/150\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 12.3934 - acc: 0.3120 - val_loss: 12.0445 - val_acc: 0.3290\n",
      "Epoch 7/150\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 11.7280 - acc: 0.3469 - val_loss: 11.3908 - val_acc: 0.3690\n",
      "Epoch 8/150\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 11.0815 - acc: 0.3880 - val_loss: 10.7551 - val_acc: 0.4060\n",
      "Epoch 9/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 10.4547 - acc: 0.4276 - val_loss: 10.1414 - val_acc: 0.4390\n",
      "Epoch 10/150\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 9.8490 - acc: 0.4687 - val_loss: 9.5489 - val_acc: 0.4730\n",
      "Epoch 11/150\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 9.2660 - acc: 0.5155 - val_loss: 8.9807 - val_acc: 0.5100\n",
      "Epoch 12/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 8.7067 - acc: 0.5435 - val_loss: 8.4321 - val_acc: 0.5310\n",
      "Epoch 13/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 8.1706 - acc: 0.5732 - val_loss: 7.9101 - val_acc: 0.5620\n",
      "Epoch 14/150\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 7.6581 - acc: 0.5965 - val_loss: 7.4108 - val_acc: 0.5860\n",
      "Epoch 15/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 7.1692 - acc: 0.6169 - val_loss: 6.9353 - val_acc: 0.6170\n",
      "Epoch 16/150\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 6.7033 - acc: 0.6363 - val_loss: 6.4825 - val_acc: 0.6230\n",
      "Epoch 17/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 6.2611 - acc: 0.6452 - val_loss: 6.0517 - val_acc: 0.6390\n",
      "Epoch 18/150\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 5.8425 - acc: 0.6572 - val_loss: 5.6472 - val_acc: 0.6550\n",
      "Epoch 19/150\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 5.4467 - acc: 0.6672 - val_loss: 5.2623 - val_acc: 0.6580\n",
      "Epoch 20/150\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 5.0752 - acc: 0.6715 - val_loss: 4.9083 - val_acc: 0.6710\n",
      "Epoch 21/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 4.7276 - acc: 0.6767 - val_loss: 4.5697 - val_acc: 0.6750\n",
      "Epoch 22/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 4.4027 - acc: 0.6848 - val_loss: 4.2590 - val_acc: 0.6650\n",
      "Epoch 23/150\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 4.1012 - acc: 0.6857 - val_loss: 3.9645 - val_acc: 0.6810\n",
      "Epoch 24/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 3.8222 - acc: 0.6923 - val_loss: 3.6982 - val_acc: 0.6830\n",
      "Epoch 25/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 3.5656 - acc: 0.6929 - val_loss: 3.4519 - val_acc: 0.6850\n",
      "Epoch 26/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 3.3304 - acc: 0.6988 - val_loss: 3.2304 - val_acc: 0.6900\n",
      "Epoch 27/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 3.1181 - acc: 0.6991 - val_loss: 3.0274 - val_acc: 0.6960\n",
      "Epoch 28/150\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 2.9268 - acc: 0.6999 - val_loss: 2.8462 - val_acc: 0.6940\n",
      "Epoch 29/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.7564 - acc: 0.7031 - val_loss: 2.6851 - val_acc: 0.6880\n",
      "Epoch 30/150\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.6075 - acc: 0.7008 - val_loss: 2.5454 - val_acc: 0.6910\n",
      "Epoch 31/150\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.4774 - acc: 0.6999 - val_loss: 2.4233 - val_acc: 0.6900\n",
      "Epoch 32/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.3670 - acc: 0.7008 - val_loss: 2.3242 - val_acc: 0.6930\n",
      "Epoch 33/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.2754 - acc: 0.6973 - val_loss: 2.2409 - val_acc: 0.6940\n",
      "Epoch 34/150\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 2.2010 - acc: 0.6988 - val_loss: 2.1721 - val_acc: 0.6930\n",
      "Epoch 35/150\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 2.1424 - acc: 0.6969 - val_loss: 2.1239 - val_acc: 0.6870\n",
      "Epoch 36/150\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 2.0986 - acc: 0.6964 - val_loss: 2.0854 - val_acc: 0.6900\n",
      "Epoch 37/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.0649 - acc: 0.6948 - val_loss: 2.0545 - val_acc: 0.6920\n",
      "Epoch 38/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.0382 - acc: 0.6965 - val_loss: 2.0292 - val_acc: 0.6910\n",
      "Epoch 39/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.0149 - acc: 0.6933 - val_loss: 2.0069 - val_acc: 0.6920\n",
      "Epoch 40/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.9939 - acc: 0.6952 - val_loss: 1.9845 - val_acc: 0.6960\n",
      "Epoch 41/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.9755 - acc: 0.6935 - val_loss: 1.9660 - val_acc: 0.6930\n",
      "Epoch 42/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.9575 - acc: 0.6951 - val_loss: 1.9532 - val_acc: 0.6930\n",
      "Epoch 43/150\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.9407 - acc: 0.6959 - val_loss: 1.9357 - val_acc: 0.6850\n",
      "Epoch 44/150\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.9250 - acc: 0.6949 - val_loss: 1.9228 - val_acc: 0.6940\n",
      "Epoch 45/150\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.9104 - acc: 0.6940 - val_loss: 1.9042 - val_acc: 0.6880\n",
      "Epoch 46/150\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.8960 - acc: 0.6947 - val_loss: 1.8913 - val_acc: 0.6950\n",
      "Epoch 47/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.8825 - acc: 0.6951 - val_loss: 1.8782 - val_acc: 0.6930\n",
      "Epoch 48/150\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.8697 - acc: 0.6949 - val_loss: 1.8644 - val_acc: 0.6940\n",
      "Epoch 49/150\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.8571 - acc: 0.6964 - val_loss: 1.8529 - val_acc: 0.6910\n",
      "Epoch 50/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8448 - acc: 0.6976 - val_loss: 1.8422 - val_acc: 0.6910\n",
      "Epoch 51/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8326 - acc: 0.6965 - val_loss: 1.8314 - val_acc: 0.6900\n",
      "Epoch 52/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8215 - acc: 0.6975 - val_loss: 1.8216 - val_acc: 0.6920\n",
      "Epoch 53/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8105 - acc: 0.6980 - val_loss: 1.8064 - val_acc: 0.6960\n",
      "Epoch 54/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7994 - acc: 0.6976 - val_loss: 1.7953 - val_acc: 0.7000\n",
      "Epoch 55/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7890 - acc: 0.6991 - val_loss: 1.7856 - val_acc: 0.6960\n",
      "Epoch 56/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7786 - acc: 0.6991 - val_loss: 1.7776 - val_acc: 0.6960\n",
      "Epoch 57/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7682 - acc: 0.7001 - val_loss: 1.7679 - val_acc: 0.6950\n",
      "Epoch 58/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.7589 - acc: 0.7004 - val_loss: 1.7572 - val_acc: 0.6950\n",
      "Epoch 59/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7490 - acc: 0.7011 - val_loss: 1.7502 - val_acc: 0.6930\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7403 - acc: 0.7025 - val_loss: 1.7358 - val_acc: 0.7010\n",
      "Epoch 61/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7304 - acc: 0.7029 - val_loss: 1.7304 - val_acc: 0.6940\n",
      "Epoch 62/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7215 - acc: 0.7016 - val_loss: 1.7231 - val_acc: 0.6960\n",
      "Epoch 63/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7126 - acc: 0.7021 - val_loss: 1.7122 - val_acc: 0.7000\n",
      "Epoch 64/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.7044 - acc: 0.7048 - val_loss: 1.7044 - val_acc: 0.6970\n",
      "Epoch 65/150\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6953 - acc: 0.7044 - val_loss: 1.6948 - val_acc: 0.6960\n",
      "Epoch 66/150\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6869 - acc: 0.7057 - val_loss: 1.6890 - val_acc: 0.6920\n",
      "Epoch 67/150\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.6785 - acc: 0.7055 - val_loss: 1.6773 - val_acc: 0.6970\n",
      "Epoch 68/150\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.6710 - acc: 0.7068 - val_loss: 1.6696 - val_acc: 0.7000\n",
      "Epoch 69/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6621 - acc: 0.7055 - val_loss: 1.6684 - val_acc: 0.6950\n",
      "Epoch 70/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6551 - acc: 0.7060 - val_loss: 1.6576 - val_acc: 0.6980\n",
      "Epoch 71/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6469 - acc: 0.7088 - val_loss: 1.6522 - val_acc: 0.6970\n",
      "Epoch 72/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6394 - acc: 0.7061 - val_loss: 1.6430 - val_acc: 0.6990\n",
      "Epoch 73/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.6317 - acc: 0.7080 - val_loss: 1.6351 - val_acc: 0.6960\n",
      "Epoch 74/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6240 - acc: 0.7080 - val_loss: 1.6279 - val_acc: 0.6990\n",
      "Epoch 75/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6171 - acc: 0.7088 - val_loss: 1.6190 - val_acc: 0.7040\n",
      "Epoch 76/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6097 - acc: 0.7092 - val_loss: 1.6135 - val_acc: 0.6990\n",
      "Epoch 77/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6026 - acc: 0.7092 - val_loss: 1.6075 - val_acc: 0.6980\n",
      "Epoch 78/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5962 - acc: 0.7108 - val_loss: 1.6000 - val_acc: 0.7020\n",
      "Epoch 79/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5883 - acc: 0.7115 - val_loss: 1.5923 - val_acc: 0.7030\n",
      "Epoch 80/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5815 - acc: 0.7109 - val_loss: 1.5892 - val_acc: 0.6980\n",
      "Epoch 81/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5754 - acc: 0.7112 - val_loss: 1.5781 - val_acc: 0.7000\n",
      "Epoch 82/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5680 - acc: 0.7128 - val_loss: 1.5757 - val_acc: 0.7010\n",
      "Epoch 83/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5615 - acc: 0.7129 - val_loss: 1.5693 - val_acc: 0.7020\n",
      "Epoch 84/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5553 - acc: 0.7116 - val_loss: 1.5609 - val_acc: 0.7030\n",
      "Epoch 85/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5488 - acc: 0.7141 - val_loss: 1.5549 - val_acc: 0.7000\n",
      "Epoch 86/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5419 - acc: 0.7135 - val_loss: 1.5496 - val_acc: 0.7060\n",
      "Epoch 87/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5357 - acc: 0.7128 - val_loss: 1.5408 - val_acc: 0.7000\n",
      "Epoch 88/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.5296 - acc: 0.7140 - val_loss: 1.5380 - val_acc: 0.7000\n",
      "Epoch 89/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5232 - acc: 0.7157 - val_loss: 1.5315 - val_acc: 0.7020\n",
      "Epoch 90/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5168 - acc: 0.7147 - val_loss: 1.5236 - val_acc: 0.7030\n",
      "Epoch 91/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5108 - acc: 0.7156 - val_loss: 1.5188 - val_acc: 0.7020\n",
      "Epoch 92/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.5048 - acc: 0.7157 - val_loss: 1.5113 - val_acc: 0.7020\n",
      "Epoch 93/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4988 - acc: 0.7153 - val_loss: 1.5066 - val_acc: 0.7020\n",
      "Epoch 94/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4926 - acc: 0.7176 - val_loss: 1.5025 - val_acc: 0.7020\n",
      "Epoch 95/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4869 - acc: 0.7168 - val_loss: 1.4939 - val_acc: 0.7050\n",
      "Epoch 96/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4812 - acc: 0.7168 - val_loss: 1.4911 - val_acc: 0.7060\n",
      "Epoch 97/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4760 - acc: 0.7181 - val_loss: 1.4898 - val_acc: 0.7030\n",
      "Epoch 98/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4696 - acc: 0.7179 - val_loss: 1.4813 - val_acc: 0.7030\n",
      "Epoch 99/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4641 - acc: 0.7184 - val_loss: 1.4729 - val_acc: 0.7050\n",
      "Epoch 100/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4583 - acc: 0.7187 - val_loss: 1.4701 - val_acc: 0.7020\n",
      "Epoch 101/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4526 - acc: 0.7176 - val_loss: 1.4610 - val_acc: 0.7060\n",
      "Epoch 102/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4478 - acc: 0.7189 - val_loss: 1.4567 - val_acc: 0.7080\n",
      "Epoch 103/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.4414 - acc: 0.7201 - val_loss: 1.4526 - val_acc: 0.7070\n",
      "Epoch 104/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.4369 - acc: 0.7200 - val_loss: 1.4514 - val_acc: 0.7020\n",
      "Epoch 105/150\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.4310 - acc: 0.7203 - val_loss: 1.4484 - val_acc: 0.7010\n",
      "Epoch 106/150\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.4257 - acc: 0.7212 - val_loss: 1.4396 - val_acc: 0.7060\n",
      "Epoch 107/150\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4201 - acc: 0.7223 - val_loss: 1.4348 - val_acc: 0.7080\n",
      "Epoch 108/150\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4146 - acc: 0.7233 - val_loss: 1.4318 - val_acc: 0.7060\n",
      "Epoch 109/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4101 - acc: 0.7227 - val_loss: 1.4235 - val_acc: 0.7030\n",
      "Epoch 110/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4041 - acc: 0.7229 - val_loss: 1.4169 - val_acc: 0.7060\n",
      "Epoch 111/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3995 - acc: 0.7225 - val_loss: 1.4125 - val_acc: 0.7050\n",
      "Epoch 112/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3945 - acc: 0.7237 - val_loss: 1.4211 - val_acc: 0.7040\n",
      "Epoch 113/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3899 - acc: 0.7241 - val_loss: 1.4069 - val_acc: 0.7040\n",
      "Epoch 114/150\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.3847 - acc: 0.7245 - val_loss: 1.3962 - val_acc: 0.7040\n",
      "Epoch 115/150\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3792 - acc: 0.7252 - val_loss: 1.3967 - val_acc: 0.7040\n",
      "Epoch 116/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3746 - acc: 0.7265 - val_loss: 1.3903 - val_acc: 0.7050\n",
      "Epoch 117/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3699 - acc: 0.7256 - val_loss: 1.3905 - val_acc: 0.7040\n",
      "Epoch 118/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3654 - acc: 0.7243 - val_loss: 1.3817 - val_acc: 0.7050\n",
      "Epoch 119/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3604 - acc: 0.7241 - val_loss: 1.3752 - val_acc: 0.7050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3555 - acc: 0.7271 - val_loss: 1.3731 - val_acc: 0.7050\n",
      "Epoch 121/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3513 - acc: 0.7257 - val_loss: 1.3655 - val_acc: 0.7050\n",
      "Epoch 122/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3460 - acc: 0.7265 - val_loss: 1.3638 - val_acc: 0.7030\n",
      "Epoch 123/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3414 - acc: 0.7283 - val_loss: 1.3603 - val_acc: 0.7050\n",
      "Epoch 124/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3372 - acc: 0.7261 - val_loss: 1.3547 - val_acc: 0.7070\n",
      "Epoch 125/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3322 - acc: 0.7292 - val_loss: 1.3518 - val_acc: 0.7050\n",
      "Epoch 126/150\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3288 - acc: 0.7284 - val_loss: 1.3453 - val_acc: 0.7090\n",
      "Epoch 127/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3239 - acc: 0.7281 - val_loss: 1.3475 - val_acc: 0.7030\n",
      "Epoch 128/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.3195 - acc: 0.7287 - val_loss: 1.3397 - val_acc: 0.7170\n",
      "Epoch 129/150\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.3156 - acc: 0.7300 - val_loss: 1.3349 - val_acc: 0.7100\n",
      "Epoch 130/150\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.3107 - acc: 0.7308 - val_loss: 1.3318 - val_acc: 0.7060\n",
      "Epoch 131/150\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.3069 - acc: 0.7329 - val_loss: 1.3379 - val_acc: 0.7060\n",
      "Epoch 132/150\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.3029 - acc: 0.7316 - val_loss: 1.3211 - val_acc: 0.7120\n",
      "Epoch 133/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2980 - acc: 0.7316 - val_loss: 1.3170 - val_acc: 0.7070\n",
      "Epoch 134/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2941 - acc: 0.7323 - val_loss: 1.3138 - val_acc: 0.7080\n",
      "Epoch 135/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2898 - acc: 0.7325 - val_loss: 1.3144 - val_acc: 0.7080\n",
      "Epoch 136/150\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2861 - acc: 0.7329 - val_loss: 1.3077 - val_acc: 0.7060\n",
      "Epoch 137/150\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2821 - acc: 0.7323 - val_loss: 1.3037 - val_acc: 0.7090\n",
      "Epoch 138/150\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2781 - acc: 0.7341 - val_loss: 1.3057 - val_acc: 0.7080\n",
      "Epoch 139/150\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2751 - acc: 0.7312 - val_loss: 1.3006 - val_acc: 0.7110\n",
      "Epoch 140/150\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2704 - acc: 0.7335 - val_loss: 1.2945 - val_acc: 0.7060\n",
      "Epoch 141/150\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2665 - acc: 0.7336 - val_loss: 1.2870 - val_acc: 0.7100\n",
      "Epoch 142/150\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2624 - acc: 0.7356 - val_loss: 1.2857 - val_acc: 0.7120\n",
      "Epoch 143/150\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2593 - acc: 0.7348 - val_loss: 1.2803 - val_acc: 0.7150\n",
      "Epoch 144/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2552 - acc: 0.7344 - val_loss: 1.2782 - val_acc: 0.7130\n",
      "Epoch 145/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2511 - acc: 0.7351 - val_loss: 1.2761 - val_acc: 0.7130\n",
      "Epoch 146/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2478 - acc: 0.7352 - val_loss: 1.2742 - val_acc: 0.7160\n",
      "Epoch 147/150\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2448 - acc: 0.7367 - val_loss: 1.2696 - val_acc: 0.7160\n",
      "Epoch 148/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2416 - acc: 0.7364 - val_loss: 1.2685 - val_acc: 0.7050\n",
      "Epoch 149/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2371 - acc: 0.7352 - val_loss: 1.2631 - val_acc: 0.7130\n",
      "Epoch 150/150\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.2340 - acc: 0.7359 - val_loss: 1.2573 - val_acc: 0.7170\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "L1_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "L1_model.add(layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,)))\n",
    "\n",
    "# Add a hidden layer\n",
    "L1_model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "L1_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L1_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model \n",
    "L1_model_val = L1_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training as well as the validation accuracy for the L1 model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "L1_model_dict = L1_model_val.history\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, acc_values, label='Training acc L1')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc L1')\n",
    "ax.set_title('Training & validation accuracy with L1 regularization')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy isn't still that good. Next, experiment with dropout regularization to see if it offers any advantages. \n",
    "\n",
    "\n",
    "## Dropout Regularization \n",
    "\n",
    "It's time to try another technique: applying dropout to layers. As discussed in the earlier lesson, this involves setting a certain proportion of units in each layer to zero. In the following cell: \n",
    "\n",
    "- Apply a dropout rate of 30% to the input layer \n",
    "- Add a first hidden layer with 50 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the first hidden layer \n",
    "- Add a second hidden layer with 25 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the second hidden layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/150\n",
      "7500/7500 [==============================] - 1s 126us/step - loss: 1.9833 - acc: 0.1384 - val_loss: 1.9370 - val_acc: 0.1700\n",
      "Epoch 2/150\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.9616 - acc: 0.1485 - val_loss: 1.9278 - val_acc: 0.1980\n",
      "Epoch 3/150\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.9432 - acc: 0.1691 - val_loss: 1.9199 - val_acc: 0.2090\n",
      "Epoch 4/150\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 1.9359 - acc: 0.1732 - val_loss: 1.9135 - val_acc: 0.2200\n",
      "Epoch 5/150\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.9268 - acc: 0.1852 - val_loss: 1.9076 - val_acc: 0.2140\n",
      "Epoch 6/150\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.9194 - acc: 0.1971 - val_loss: 1.9013 - val_acc: 0.2050\n",
      "Epoch 7/150\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.9105 - acc: 0.2017 - val_loss: 1.8944 - val_acc: 0.2040\n",
      "Epoch 8/150\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.9043 - acc: 0.2033 - val_loss: 1.8868 - val_acc: 0.2140\n",
      "Epoch 9/150\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.9017 - acc: 0.2167 - val_loss: 1.8795 - val_acc: 0.2180\n",
      "Epoch 10/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.8934 - acc: 0.2161 - val_loss: 1.8701 - val_acc: 0.2200\n",
      "Epoch 11/150\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.8870 - acc: 0.2217 - val_loss: 1.8603 - val_acc: 0.2370\n",
      "Epoch 12/150\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.8778 - acc: 0.2293 - val_loss: 1.8484 - val_acc: 0.2570\n",
      "Epoch 13/150\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.8681 - acc: 0.2341 - val_loss: 1.8345 - val_acc: 0.2800\n",
      "Epoch 14/150\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.8588 - acc: 0.2404 - val_loss: 1.8196 - val_acc: 0.2850\n",
      "Epoch 15/150\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.8461 - acc: 0.2579 - val_loss: 1.8041 - val_acc: 0.2920\n",
      "Epoch 16/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.8299 - acc: 0.2605 - val_loss: 1.7853 - val_acc: 0.3110\n",
      "Epoch 17/150\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.8273 - acc: 0.2664 - val_loss: 1.7683 - val_acc: 0.3310\n",
      "Epoch 18/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.8049 - acc: 0.2789 - val_loss: 1.7482 - val_acc: 0.3430\n",
      "Epoch 19/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.7903 - acc: 0.2843 - val_loss: 1.7272 - val_acc: 0.3600\n",
      "Epoch 20/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.7789 - acc: 0.2941 - val_loss: 1.7059 - val_acc: 0.3660\n",
      "Epoch 21/150\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.7616 - acc: 0.2993 - val_loss: 1.6845 - val_acc: 0.3730\n",
      "Epoch 22/150\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.7465 - acc: 0.3060 - val_loss: 1.6627 - val_acc: 0.3850\n",
      "Epoch 23/150\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.7316 - acc: 0.3159 - val_loss: 1.6417 - val_acc: 0.3990\n",
      "Epoch 24/150\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.7146 - acc: 0.3312 - val_loss: 1.6195 - val_acc: 0.4120\n",
      "Epoch 25/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.6946 - acc: 0.3371 - val_loss: 1.5969 - val_acc: 0.4330\n",
      "Epoch 26/150\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 1.6807 - acc: 0.3479 - val_loss: 1.5725 - val_acc: 0.4580\n",
      "Epoch 27/150\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.6653 - acc: 0.3563 - val_loss: 1.5499 - val_acc: 0.4640\n",
      "Epoch 28/150\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.6401 - acc: 0.3679 - val_loss: 1.5250 - val_acc: 0.4790\n",
      "Epoch 29/150\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.6270 - acc: 0.3743 - val_loss: 1.5036 - val_acc: 0.4840\n",
      "Epoch 30/150\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.6152 - acc: 0.3793 - val_loss: 1.4800 - val_acc: 0.4940\n",
      "Epoch 31/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.6003 - acc: 0.3837 - val_loss: 1.4571 - val_acc: 0.5130\n",
      "Epoch 32/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.5813 - acc: 0.3945 - val_loss: 1.4348 - val_acc: 0.5260\n",
      "Epoch 33/150\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.5694 - acc: 0.3973 - val_loss: 1.4124 - val_acc: 0.5500\n",
      "Epoch 34/150\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.5471 - acc: 0.4095 - val_loss: 1.3904 - val_acc: 0.5580\n",
      "Epoch 35/150\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.5273 - acc: 0.4119 - val_loss: 1.3701 - val_acc: 0.5660\n",
      "Epoch 36/150\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.5182 - acc: 0.4195 - val_loss: 1.3505 - val_acc: 0.5730\n",
      "Epoch 37/150\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.4934 - acc: 0.4335 - val_loss: 1.3255 - val_acc: 0.5880\n",
      "Epoch 38/150\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 1.4813 - acc: 0.4376 - val_loss: 1.3052 - val_acc: 0.6070\n",
      "Epoch 39/150\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.4885 - acc: 0.4307 - val_loss: 1.2895 - val_acc: 0.6130\n",
      "Epoch 40/150\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.4682 - acc: 0.4345 - val_loss: 1.2722 - val_acc: 0.6180\n",
      "Epoch 41/150\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.4361 - acc: 0.4521 - val_loss: 1.2514 - val_acc: 0.6250\n",
      "Epoch 42/150\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.4329 - acc: 0.4573 - val_loss: 1.2329 - val_acc: 0.6250\n",
      "Epoch 43/150\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.4288 - acc: 0.4592 - val_loss: 1.2174 - val_acc: 0.6290\n",
      "Epoch 44/150\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.4077 - acc: 0.4643 - val_loss: 1.1996 - val_acc: 0.6370\n",
      "Epoch 45/150\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.3926 - acc: 0.4660 - val_loss: 1.1855 - val_acc: 0.6390\n",
      "Epoch 46/150\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.3830 - acc: 0.4736 - val_loss: 1.1700 - val_acc: 0.6460\n",
      "Epoch 47/150\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.3807 - acc: 0.4772 - val_loss: 1.1554 - val_acc: 0.6490\n",
      "Epoch 48/150\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.3604 - acc: 0.4841 - val_loss: 1.1425 - val_acc: 0.6500\n",
      "Epoch 49/150\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.3451 - acc: 0.4936 - val_loss: 1.1276 - val_acc: 0.6620\n",
      "Epoch 50/150\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.3260 - acc: 0.4959 - val_loss: 1.1110 - val_acc: 0.6650\n",
      "Epoch 51/150\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.3184 - acc: 0.4992 - val_loss: 1.0972 - val_acc: 0.6650\n",
      "Epoch 52/150\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 1.3150 - acc: 0.4993 - val_loss: 1.0844 - val_acc: 0.6690\n",
      "Epoch 53/150\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.3084 - acc: 0.5084 - val_loss: 1.0734 - val_acc: 0.6730\n",
      "Epoch 54/150\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.2932 - acc: 0.5067 - val_loss: 1.0637 - val_acc: 0.6690\n",
      "Epoch 55/150\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.2859 - acc: 0.5077 - val_loss: 1.0531 - val_acc: 0.6710\n",
      "Epoch 56/150\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.2859 - acc: 0.5208 - val_loss: 1.0429 - val_acc: 0.6780\n",
      "Epoch 57/150\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.2695 - acc: 0.5192 - val_loss: 1.0306 - val_acc: 0.6830\n",
      "Epoch 58/150\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.2457 - acc: 0.5332 - val_loss: 1.0220 - val_acc: 0.6810\n",
      "Epoch 59/150\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.2467 - acc: 0.5229 - val_loss: 1.0105 - val_acc: 0.6850\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.2428 - acc: 0.5317 - val_loss: 1.0044 - val_acc: 0.6860\n",
      "Epoch 61/150\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.2414 - acc: 0.5273 - val_loss: 0.9960 - val_acc: 0.6880\n",
      "Epoch 62/150\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.2331 - acc: 0.5392 - val_loss: 0.9895 - val_acc: 0.6870\n",
      "Epoch 63/150\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.2252 - acc: 0.5312 - val_loss: 0.9826 - val_acc: 0.6970\n",
      "Epoch 64/150\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.2058 - acc: 0.5441 - val_loss: 0.9710 - val_acc: 0.6990\n",
      "Epoch 65/150\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.2114 - acc: 0.5348 - val_loss: 0.9632 - val_acc: 0.6980\n",
      "Epoch 66/150\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.1982 - acc: 0.5471 - val_loss: 0.9545 - val_acc: 0.7050\n",
      "Epoch 67/150\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.2082 - acc: 0.5373 - val_loss: 0.9517 - val_acc: 0.7030\n",
      "Epoch 68/150\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 1.1881 - acc: 0.5564 - val_loss: 0.9421 - val_acc: 0.7040\n",
      "Epoch 69/150\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.1880 - acc: 0.5561 - val_loss: 0.9378 - val_acc: 0.7070\n",
      "Epoch 70/150\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1853 - acc: 0.5545 - val_loss: 0.9298 - val_acc: 0.7070\n",
      "Epoch 71/150\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.1670 - acc: 0.5580 - val_loss: 0.9216 - val_acc: 0.7090\n",
      "Epoch 72/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1652 - acc: 0.5689 - val_loss: 0.9172 - val_acc: 0.7120\n",
      "Epoch 73/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.1599 - acc: 0.5668 - val_loss: 0.9090 - val_acc: 0.7100\n",
      "Epoch 74/150\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.1485 - acc: 0.5653 - val_loss: 0.9015 - val_acc: 0.7100\n",
      "Epoch 75/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.1416 - acc: 0.5736 - val_loss: 0.8974 - val_acc: 0.7170\n",
      "Epoch 76/150\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1270 - acc: 0.5787 - val_loss: 0.8902 - val_acc: 0.7150\n",
      "Epoch 77/150\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1318 - acc: 0.5699 - val_loss: 0.8847 - val_acc: 0.7180\n",
      "Epoch 78/150\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1174 - acc: 0.5749 - val_loss: 0.8785 - val_acc: 0.7130\n",
      "Epoch 79/150\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.1245 - acc: 0.5740 - val_loss: 0.8753 - val_acc: 0.7180\n",
      "Epoch 80/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.1203 - acc: 0.5736 - val_loss: 0.8699 - val_acc: 0.7120\n",
      "Epoch 81/150\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.1042 - acc: 0.5881 - val_loss: 0.8620 - val_acc: 0.7160\n",
      "Epoch 82/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.1022 - acc: 0.5879 - val_loss: 0.8595 - val_acc: 0.7180\n",
      "Epoch 83/150\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.1050 - acc: 0.5872 - val_loss: 0.8557 - val_acc: 0.7190\n",
      "Epoch 84/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0889 - acc: 0.5949 - val_loss: 0.8498 - val_acc: 0.7180\n",
      "Epoch 85/150\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.0847 - acc: 0.5941 - val_loss: 0.8438 - val_acc: 0.7230\n",
      "Epoch 86/150\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.0826 - acc: 0.5979 - val_loss: 0.8420 - val_acc: 0.7220\n",
      "Epoch 87/150\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0833 - acc: 0.5995 - val_loss: 0.8390 - val_acc: 0.7200\n",
      "Epoch 88/150\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.0613 - acc: 0.6019 - val_loss: 0.8321 - val_acc: 0.7200\n",
      "Epoch 89/150\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 1.0649 - acc: 0.6069 - val_loss: 0.8306 - val_acc: 0.7240\n",
      "Epoch 90/150\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.0495 - acc: 0.6079 - val_loss: 0.8215 - val_acc: 0.7210\n",
      "Epoch 91/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0572 - acc: 0.6029 - val_loss: 0.8167 - val_acc: 0.7220\n",
      "Epoch 92/150\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.0643 - acc: 0.5953 - val_loss: 0.8165 - val_acc: 0.7240\n",
      "Epoch 93/150\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0509 - acc: 0.6081 - val_loss: 0.8115 - val_acc: 0.7220\n",
      "Epoch 94/150\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.0449 - acc: 0.6128 - val_loss: 0.8072 - val_acc: 0.7220\n",
      "Epoch 95/150\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0547 - acc: 0.6060 - val_loss: 0.8052 - val_acc: 0.7240\n",
      "Epoch 96/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.0424 - acc: 0.6088 - val_loss: 0.8010 - val_acc: 0.7220\n",
      "Epoch 97/150\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.0329 - acc: 0.6137 - val_loss: 0.7993 - val_acc: 0.7230\n",
      "Epoch 98/150\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.0268 - acc: 0.6151 - val_loss: 0.7944 - val_acc: 0.7220\n",
      "Epoch 99/150\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0299 - acc: 0.6177 - val_loss: 0.7923 - val_acc: 0.7240\n",
      "Epoch 100/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.0223 - acc: 0.6165 - val_loss: 0.7901 - val_acc: 0.7240\n",
      "Epoch 101/150\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0219 - acc: 0.6196 - val_loss: 0.7861 - val_acc: 0.7260\n",
      "Epoch 102/150\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.0250 - acc: 0.6147 - val_loss: 0.7840 - val_acc: 0.7280\n",
      "Epoch 103/150\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.0082 - acc: 0.6229 - val_loss: 0.7793 - val_acc: 0.7240\n",
      "Epoch 104/150\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.9998 - acc: 0.6312 - val_loss: 0.7755 - val_acc: 0.7290\n",
      "Epoch 105/150\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 0.9984 - acc: 0.6293 - val_loss: 0.7716 - val_acc: 0.7240\n",
      "Epoch 106/150\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9997 - acc: 0.6227 - val_loss: 0.7698 - val_acc: 0.7310\n",
      "Epoch 107/150\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9960 - acc: 0.6260 - val_loss: 0.7672 - val_acc: 0.7270\n",
      "Epoch 108/150\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.9923 - acc: 0.6304 - val_loss: 0.7654 - val_acc: 0.7320\n",
      "Epoch 109/150\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9959 - acc: 0.6220 - val_loss: 0.7628 - val_acc: 0.7300\n",
      "Epoch 110/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9869 - acc: 0.6304 - val_loss: 0.7595 - val_acc: 0.7310\n",
      "Epoch 111/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9832 - acc: 0.6317 - val_loss: 0.7569 - val_acc: 0.7310\n",
      "Epoch 112/150\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9958 - acc: 0.6289 - val_loss: 0.7538 - val_acc: 0.7310\n",
      "Epoch 113/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9558 - acc: 0.6487 - val_loss: 0.7529 - val_acc: 0.7290\n",
      "Epoch 114/150\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 0.9686 - acc: 0.6353 - val_loss: 0.7485 - val_acc: 0.7320\n",
      "Epoch 115/150\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.9809 - acc: 0.6307 - val_loss: 0.7465 - val_acc: 0.7310\n",
      "Epoch 116/150\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 0.9810 - acc: 0.6364 - val_loss: 0.7453 - val_acc: 0.7330\n",
      "Epoch 117/150\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 0.9669 - acc: 0.6379 - val_loss: 0.7430 - val_acc: 0.7340\n",
      "Epoch 118/150\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9642 - acc: 0.6384 - val_loss: 0.7379 - val_acc: 0.7360\n",
      "Epoch 119/150\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9695 - acc: 0.6417 - val_loss: 0.7387 - val_acc: 0.7310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9643 - acc: 0.6417 - val_loss: 0.7354 - val_acc: 0.7330\n",
      "Epoch 121/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9496 - acc: 0.6555 - val_loss: 0.7294 - val_acc: 0.7320\n",
      "Epoch 122/150\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9475 - acc: 0.6451 - val_loss: 0.7312 - val_acc: 0.7330\n",
      "Epoch 123/150\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9374 - acc: 0.6491 - val_loss: 0.7322 - val_acc: 0.7290\n",
      "Epoch 124/150\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.9420 - acc: 0.6535 - val_loss: 0.7279 - val_acc: 0.7300\n",
      "Epoch 125/150\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9402 - acc: 0.6612 - val_loss: 0.7261 - val_acc: 0.7310\n",
      "Epoch 126/150\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.9445 - acc: 0.6492 - val_loss: 0.7251 - val_acc: 0.7350\n",
      "Epoch 127/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9284 - acc: 0.6495 - val_loss: 0.7212 - val_acc: 0.7380\n",
      "Epoch 128/150\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.9213 - acc: 0.6600 - val_loss: 0.7196 - val_acc: 0.7370\n",
      "Epoch 129/150\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9200 - acc: 0.6540 - val_loss: 0.7168 - val_acc: 0.7330\n",
      "Epoch 130/150\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.9328 - acc: 0.6499 - val_loss: 0.7139 - val_acc: 0.7340\n",
      "Epoch 131/150\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9166 - acc: 0.6576 - val_loss: 0.7114 - val_acc: 0.7340\n",
      "Epoch 132/150\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9180 - acc: 0.6504 - val_loss: 0.7135 - val_acc: 0.7370\n",
      "Epoch 133/150\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.9183 - acc: 0.6552 - val_loss: 0.7085 - val_acc: 0.7380\n",
      "Epoch 134/150\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9119 - acc: 0.6596 - val_loss: 0.7066 - val_acc: 0.7400\n",
      "Epoch 135/150\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9001 - acc: 0.6689 - val_loss: 0.7023 - val_acc: 0.7420\n",
      "Epoch 136/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9104 - acc: 0.6624 - val_loss: 0.7048 - val_acc: 0.7370\n",
      "Epoch 137/150\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8978 - acc: 0.6612 - val_loss: 0.7009 - val_acc: 0.7360\n",
      "Epoch 138/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9019 - acc: 0.6609 - val_loss: 0.6983 - val_acc: 0.7380\n",
      "Epoch 139/150\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9078 - acc: 0.6603 - val_loss: 0.6992 - val_acc: 0.7400\n",
      "Epoch 140/150\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8966 - acc: 0.6721 - val_loss: 0.6951 - val_acc: 0.7380\n",
      "Epoch 141/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9071 - acc: 0.6627 - val_loss: 0.6950 - val_acc: 0.7400\n",
      "Epoch 142/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8802 - acc: 0.6719 - val_loss: 0.6948 - val_acc: 0.7410\n",
      "Epoch 143/150\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8989 - acc: 0.6689 - val_loss: 0.6927 - val_acc: 0.7440\n",
      "Epoch 144/150\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8955 - acc: 0.6691 - val_loss: 0.6907 - val_acc: 0.7420\n",
      "Epoch 145/150\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8796 - acc: 0.6697 - val_loss: 0.6881 - val_acc: 0.7420\n",
      "Epoch 146/150\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8872 - acc: 0.6723 - val_loss: 0.6877 - val_acc: 0.7390\n",
      "Epoch 147/150\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8895 - acc: 0.6649 - val_loss: 0.6878 - val_acc: 0.7390\n",
      "Epoch 148/150\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8804 - acc: 0.6767 - val_loss: 0.6844 - val_acc: 0.7470\n",
      "Epoch 149/150\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8633 - acc: 0.6769 - val_loss: 0.6815 - val_acc: 0.7480\n",
      "Epoch 150/150\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8836 - acc: 0.6715 - val_loss: 0.6806 - val_acc: 0.7440\n"
     ]
    }
   ],
   "source": [
    "#  This cell may take about a minute to run\n",
    "random.seed(123)\n",
    "dropout_model = models.Sequential()\n",
    "\n",
    "# Implement dropout to the input layer\n",
    "# NOTE: This is where you define the number of units in the input layer\n",
    "dropout_model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "\n",
    "# Add the first hidden layer\n",
    "dropout_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Implement dropout to the first hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the second hidden layer\n",
    "dropout_model.add(layers.Dense(25, activation='relu'))\n",
    "\n",
    "# Implement dropout to the second hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the output layer\n",
    "dropout_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "dropout_model.compile(optimizer='SGD', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "dropout_model_val = dropout_model.fit(X_train_tokens, \n",
    "                                      y_train_lb, \n",
    "                                      epochs=150, \n",
    "                                      batch_size=256, \n",
    "                                      validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 82us/step\n",
      "Training Loss: 0.581 \n",
      "Training Accuracy: 0.795\n",
      "----------\n",
      "1500/1500 [==============================] - 0s 77us/step\n",
      "Test Loss: 0.64 \n",
      "Test Accuracy: 0.774\n"
     ]
    }
   ],
   "source": [
    "results_train = dropout_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = dropout_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again, and the training and test accuracy are very close!  \n",
    "\n",
    "## Bigger Data? \n",
    "\n",
    "Finally, let's examine if we can improve the model's performance just by adding more data. We've quadrapled the sample dataset from 10,000 to 40,000 observations, and all you need to do is run the code! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigger_sample = df.sample(40000, random_state=123)\n",
    "\n",
    "X = df['Consumer complaint narrative']\n",
    "y = df['Product']\n",
    "\n",
    "# Train-test split\n",
    "X_train_bigger, X_test_bigger, y_train_bigger, y_test_bigger = train_test_split(X, \n",
    "                                                                                y, \n",
    "                                                                                test_size=6000, \n",
    "                                                                                random_state=42)\n",
    "\n",
    "# Validation set\n",
    "X_train_final_bigger, X_val_bigger, y_train_final_bigger, y_val_bigger = train_test_split(X_train_bigger, \n",
    "                                                                                          y_train_bigger, \n",
    "                                                                                          test_size=4000, \n",
    "                                                                                          random_state=42)\n",
    "\n",
    "\n",
    "# One-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train_final_bigger)\n",
    "\n",
    "X_train_tokens_bigger = tokenizer.texts_to_matrix(X_train_final_bigger, mode='binary')\n",
    "X_val_tokens_bigger = tokenizer.texts_to_matrix(X_val_bigger, mode='binary')\n",
    "X_test_tokens_bigger = tokenizer.texts_to_matrix(X_test_bigger, mode='binary')\n",
    "\n",
    "# One-hot encoding of products\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final_bigger)\n",
    "\n",
    "y_train_lb_bigger = to_categorical(lb.transform(y_train_final_bigger))[:, :, 1]\n",
    "y_val_lb_bigger = to_categorical(lb.transform(y_val_bigger))[:, :, 1]\n",
    "y_test_lb_bigger = to_categorical(lb.transform(y_test_bigger))[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 4000 samples\n",
      "Epoch 1/150\n",
      "50000/50000 [==============================] - 3s 62us/step - loss: 1.8659 - acc: 0.2627 - val_loss: 1.7603 - val_acc: 0.3707\n",
      "Epoch 2/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.6046 - acc: 0.4538 - val_loss: 1.4481 - val_acc: 0.5340\n",
      "Epoch 3/150\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 1.2850 - acc: 0.5928 - val_loss: 1.1615 - val_acc: 0.6332\n",
      "Epoch 4/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.0421 - acc: 0.6649 - val_loss: 0.9763 - val_acc: 0.6778\n",
      "Epoch 5/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.8892 - acc: 0.7028 - val_loss: 0.8591 - val_acc: 0.7085\n",
      "Epoch 6/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.7941 - acc: 0.7262 - val_loss: 0.7864 - val_acc: 0.7285\n",
      "Epoch 7/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.7315 - acc: 0.7430 - val_loss: 0.7384 - val_acc: 0.7400\n",
      "Epoch 8/150\n",
      "50000/50000 [==============================] - 2s 49us/step - loss: 0.6876 - acc: 0.7556 - val_loss: 0.7034 - val_acc: 0.7505\n",
      "Epoch 9/150\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.6552 - acc: 0.7655 - val_loss: 0.6779 - val_acc: 0.7543\n",
      "Epoch 10/150\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.6299 - acc: 0.7723 - val_loss: 0.6589 - val_acc: 0.7598\n",
      "Epoch 11/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.6097 - acc: 0.7794 - val_loss: 0.6437 - val_acc: 0.7658\n",
      "Epoch 12/150\n",
      "50000/50000 [==============================] - 3s 62us/step - loss: 0.5928 - acc: 0.7859 - val_loss: 0.6308 - val_acc: 0.7708\n",
      "Epoch 13/150\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.5786 - acc: 0.7907 - val_loss: 0.6210 - val_acc: 0.7730\n",
      "Epoch 14/150\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.5659 - acc: 0.7955 - val_loss: 0.6121 - val_acc: 0.7762\n",
      "Epoch 15/150\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.5549 - acc: 0.7993 - val_loss: 0.6043 - val_acc: 0.7800\n",
      "Epoch 16/150\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 0.5446 - acc: 0.8043 - val_loss: 0.5990 - val_acc: 0.7830\n",
      "Epoch 17/150\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.5359 - acc: 0.8071 - val_loss: 0.5899 - val_acc: 0.7858\n",
      "Epoch 18/150\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.5278 - acc: 0.8106 - val_loss: 0.5868 - val_acc: 0.7880\n",
      "Epoch 19/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.5202 - acc: 0.8139 - val_loss: 0.5882 - val_acc: 0.7853\n",
      "Epoch 20/150\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.5132 - acc: 0.8162 - val_loss: 0.5780 - val_acc: 0.7900\n",
      "Epoch 21/150\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.5066 - acc: 0.8192 - val_loss: 0.5779 - val_acc: 0.7898\n",
      "Epoch 22/150\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.5007 - acc: 0.8222 - val_loss: 0.5686 - val_acc: 0.7950\n",
      "Epoch 23/150\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.4948 - acc: 0.8227 - val_loss: 0.5661 - val_acc: 0.7947\n",
      "Epoch 24/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.4894 - acc: 0.8256 - val_loss: 0.5651 - val_acc: 0.7960\n",
      "Epoch 25/150\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.4841 - acc: 0.8283 - val_loss: 0.5627 - val_acc: 0.7962\n",
      "Epoch 26/150\n",
      "50000/50000 [==============================] - 2s 50us/step - loss: 0.4793 - acc: 0.8294 - val_loss: 0.5599 - val_acc: 0.7992\n",
      "Epoch 27/150\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.4748 - acc: 0.8311 - val_loss: 0.5611 - val_acc: 0.8000\n",
      "Epoch 28/150\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.4705 - acc: 0.8340 - val_loss: 0.5554 - val_acc: 0.7982\n",
      "Epoch 29/150\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.4661 - acc: 0.8351 - val_loss: 0.5615 - val_acc: 0.8015\n",
      "Epoch 30/150\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 0.4623 - acc: 0.8367 - val_loss: 0.5551 - val_acc: 0.8025\n",
      "Epoch 31/150\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.4588 - acc: 0.8372 - val_loss: 0.5527 - val_acc: 0.8053\n",
      "Epoch 32/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.4551 - acc: 0.8395 - val_loss: 0.5554 - val_acc: 0.8020\n",
      "Epoch 33/150\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 0.4518 - acc: 0.8404 - val_loss: 0.5548 - val_acc: 0.8015\n",
      "Epoch 34/150\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.4485 - acc: 0.8419 - val_loss: 0.5482 - val_acc: 0.8062\n",
      "Epoch 35/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.4449 - acc: 0.8432 - val_loss: 0.5478 - val_acc: 0.8053\n",
      "Epoch 36/150\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.4423 - acc: 0.8440 - val_loss: 0.5484 - val_acc: 0.8058\n",
      "Epoch 37/150\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.4390 - acc: 0.8452 - val_loss: 0.5444 - val_acc: 0.8082\n",
      "Epoch 38/150\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.4360 - acc: 0.8467 - val_loss: 0.5461 - val_acc: 0.8082\n",
      "Epoch 39/150\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.4334 - acc: 0.8469 - val_loss: 0.5444 - val_acc: 0.8078\n",
      "Epoch 40/150\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.4308 - acc: 0.8487 - val_loss: 0.5445 - val_acc: 0.8082\n",
      "Epoch 41/150\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.4279 - acc: 0.8502 - val_loss: 0.5410 - val_acc: 0.8110\n",
      "Epoch 42/150\n",
      "50000/50000 [==============================] - 3s 51us/step - loss: 0.4254 - acc: 0.8512 - val_loss: 0.5419 - val_acc: 0.8125\n",
      "Epoch 43/150\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.4228 - acc: 0.8520 - val_loss: 0.5422 - val_acc: 0.8105\n",
      "Epoch 44/150\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.4205 - acc: 0.8528 - val_loss: 0.5420 - val_acc: 0.8093\n",
      "Epoch 45/150\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.4184 - acc: 0.8527 - val_loss: 0.5408 - val_acc: 0.8125\n",
      "Epoch 46/150\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.4160 - acc: 0.8549 - val_loss: 0.5448 - val_acc: 0.8083\n",
      "Epoch 47/150\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.4138 - acc: 0.8554 - val_loss: 0.5433 - val_acc: 0.8105\n",
      "Epoch 48/150\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.4116 - acc: 0.8568 - val_loss: 0.5408 - val_acc: 0.8090\n",
      "Epoch 49/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.4096 - acc: 0.8565 - val_loss: 0.5402 - val_acc: 0.8080\n",
      "Epoch 50/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.4072 - acc: 0.8578 - val_loss: 0.5401 - val_acc: 0.8108\n",
      "Epoch 51/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.4054 - acc: 0.8581 - val_loss: 0.5426 - val_acc: 0.8100\n",
      "Epoch 52/150\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.4039 - acc: 0.8593 - val_loss: 0.5427 - val_acc: 0.8105\n",
      "Epoch 53/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.4019 - acc: 0.8590 - val_loss: 0.5416 - val_acc: 0.8115\n",
      "Epoch 54/150\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3999 - acc: 0.8604 - val_loss: 0.5406 - val_acc: 0.8145\n",
      "Epoch 55/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3980 - acc: 0.8607 - val_loss: 0.5425 - val_acc: 0.8095\n",
      "Epoch 56/150\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3964 - acc: 0.8612 - val_loss: 0.5400 - val_acc: 0.8107\n",
      "Epoch 57/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3945 - acc: 0.8625 - val_loss: 0.5404 - val_acc: 0.8080\n",
      "Epoch 58/150\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3927 - acc: 0.8627 - val_loss: 0.5435 - val_acc: 0.8118\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3909 - acc: 0.8629 - val_loss: 0.5411 - val_acc: 0.8100\n",
      "Epoch 60/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3893 - acc: 0.8646 - val_loss: 0.5426 - val_acc: 0.8113\n",
      "Epoch 61/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3879 - acc: 0.8646 - val_loss: 0.5407 - val_acc: 0.8092\n",
      "Epoch 62/150\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3861 - acc: 0.8653 - val_loss: 0.5425 - val_acc: 0.8085\n",
      "Epoch 63/150\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3849 - acc: 0.8652 - val_loss: 0.5452 - val_acc: 0.8110\n",
      "Epoch 64/150\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3831 - acc: 0.8658 - val_loss: 0.5416 - val_acc: 0.8120\n",
      "Epoch 65/150\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3817 - acc: 0.8674 - val_loss: 0.5417 - val_acc: 0.8090\n",
      "Epoch 66/150\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3803 - acc: 0.8675 - val_loss: 0.5426 - val_acc: 0.8110\n",
      "Epoch 67/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3788 - acc: 0.8674 - val_loss: 0.5479 - val_acc: 0.8068\n",
      "Epoch 68/150\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3771 - acc: 0.8685 - val_loss: 0.5433 - val_acc: 0.8120\n",
      "Epoch 69/150\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3761 - acc: 0.8680 - val_loss: 0.5468 - val_acc: 0.8128\n",
      "Epoch 70/150\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 0.3745 - acc: 0.8696 - val_loss: 0.5429 - val_acc: 0.8078\n",
      "Epoch 71/150\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3728 - acc: 0.8697 - val_loss: 0.5453 - val_acc: 0.8098\n",
      "Epoch 72/150\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3719 - acc: 0.8703 - val_loss: 0.5422 - val_acc: 0.8078\n",
      "Epoch 73/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3704 - acc: 0.8704 - val_loss: 0.5454 - val_acc: 0.8115\n",
      "Epoch 74/150\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3694 - acc: 0.8709 - val_loss: 0.5506 - val_acc: 0.8075\n",
      "Epoch 75/150\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3678 - acc: 0.8702 - val_loss: 0.5516 - val_acc: 0.8077\n",
      "Epoch 76/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3668 - acc: 0.8708 - val_loss: 0.5461 - val_acc: 0.8082\n",
      "Epoch 77/150\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3653 - acc: 0.8722 - val_loss: 0.5473 - val_acc: 0.8115\n",
      "Epoch 78/150\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3642 - acc: 0.8725 - val_loss: 0.5468 - val_acc: 0.8077\n",
      "Epoch 79/150\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3630 - acc: 0.8723 - val_loss: 0.5494 - val_acc: 0.8080\n",
      "Epoch 80/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3618 - acc: 0.8732 - val_loss: 0.5527 - val_acc: 0.8070\n",
      "Epoch 81/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3607 - acc: 0.8740 - val_loss: 0.5485 - val_acc: 0.8087\n",
      "Epoch 82/150\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3593 - acc: 0.8743 - val_loss: 0.5475 - val_acc: 0.8092\n",
      "Epoch 83/150\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3583 - acc: 0.8735 - val_loss: 0.5519 - val_acc: 0.8128\n",
      "Epoch 84/150\n",
      "50000/50000 [==============================] - 3s 51us/step - loss: 0.3571 - acc: 0.8744 - val_loss: 0.5565 - val_acc: 0.8055\n",
      "Epoch 85/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3557 - acc: 0.8752 - val_loss: 0.5494 - val_acc: 0.8097\n",
      "Epoch 86/150\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3547 - acc: 0.8758 - val_loss: 0.5530 - val_acc: 0.8078\n",
      "Epoch 87/150\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3539 - acc: 0.8761 - val_loss: 0.5505 - val_acc: 0.8125\n",
      "Epoch 88/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3525 - acc: 0.8767 - val_loss: 0.5543 - val_acc: 0.8082\n",
      "Epoch 89/150\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3515 - acc: 0.8767 - val_loss: 0.5520 - val_acc: 0.8087\n",
      "Epoch 90/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3504 - acc: 0.8775 - val_loss: 0.5568 - val_acc: 0.8062\n",
      "Epoch 91/150\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3493 - acc: 0.8778 - val_loss: 0.5543 - val_acc: 0.8120\n",
      "Epoch 92/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3483 - acc: 0.8783 - val_loss: 0.5570 - val_acc: 0.8062\n",
      "Epoch 93/150\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3475 - acc: 0.8777 - val_loss: 0.5527 - val_acc: 0.8110\n",
      "Epoch 94/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3460 - acc: 0.8794 - val_loss: 0.5567 - val_acc: 0.8107\n",
      "Epoch 95/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3454 - acc: 0.8786 - val_loss: 0.5570 - val_acc: 0.8075\n",
      "Epoch 96/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3440 - acc: 0.8795 - val_loss: 0.5577 - val_acc: 0.8100\n",
      "Epoch 97/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3432 - acc: 0.8796 - val_loss: 0.5548 - val_acc: 0.8115\n",
      "Epoch 98/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3420 - acc: 0.8811 - val_loss: 0.5597 - val_acc: 0.8107\n",
      "Epoch 99/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3413 - acc: 0.8805 - val_loss: 0.5570 - val_acc: 0.8127\n",
      "Epoch 100/150\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3401 - acc: 0.8817 - val_loss: 0.5677 - val_acc: 0.8090\n",
      "Epoch 101/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3392 - acc: 0.8819 - val_loss: 0.5639 - val_acc: 0.8082\n",
      "Epoch 102/150\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3385 - acc: 0.8815 - val_loss: 0.5623 - val_acc: 0.8070\n",
      "Epoch 103/150\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3371 - acc: 0.8828 - val_loss: 0.5575 - val_acc: 0.8120\n",
      "Epoch 104/150\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 0.3362 - acc: 0.8824 - val_loss: 0.5652 - val_acc: 0.8080\n",
      "Epoch 105/150\n",
      "50000/50000 [==============================] - 2s 49us/step - loss: 0.3355 - acc: 0.8835 - val_loss: 0.5609 - val_acc: 0.8087\n",
      "Epoch 106/150\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3341 - acc: 0.8837 - val_loss: 0.5596 - val_acc: 0.8112\n",
      "Epoch 107/150\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3334 - acc: 0.8835 - val_loss: 0.5707 - val_acc: 0.8000\n",
      "Epoch 108/150\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3324 - acc: 0.8838 - val_loss: 0.5636 - val_acc: 0.8097\n",
      "Epoch 109/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3313 - acc: 0.8847 - val_loss: 0.5607 - val_acc: 0.8090\n",
      "Epoch 110/150\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3304 - acc: 0.8840 - val_loss: 0.5718 - val_acc: 0.8005\n",
      "Epoch 111/150\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3292 - acc: 0.8853 - val_loss: 0.5722 - val_acc: 0.8050\n",
      "Epoch 112/150\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3287 - acc: 0.8855 - val_loss: 0.5636 - val_acc: 0.8080\n",
      "Epoch 113/150\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3277 - acc: 0.8859 - val_loss: 0.5663 - val_acc: 0.8085\n",
      "Epoch 114/150\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3267 - acc: 0.8863 - val_loss: 0.5754 - val_acc: 0.8075\n",
      "Epoch 115/150\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3257 - acc: 0.8864 - val_loss: 0.5699 - val_acc: 0.8053\n",
      "Epoch 116/150\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3248 - acc: 0.8874 - val_loss: 0.5681 - val_acc: 0.8102\n",
      "Epoch 117/150\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3238 - acc: 0.8873 - val_loss: 0.5712 - val_acc: 0.8070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/150\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3228 - acc: 0.8878 - val_loss: 0.5670 - val_acc: 0.8095\n",
      "Epoch 119/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3220 - acc: 0.8884 - val_loss: 0.5753 - val_acc: 0.8107\n",
      "Epoch 120/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3210 - acc: 0.8881 - val_loss: 0.5710 - val_acc: 0.8080\n",
      "Epoch 121/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3203 - acc: 0.8890 - val_loss: 0.5723 - val_acc: 0.8065\n",
      "Epoch 122/150\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3191 - acc: 0.8892 - val_loss: 0.5722 - val_acc: 0.8063\n",
      "Epoch 123/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3183 - acc: 0.8890 - val_loss: 0.5774 - val_acc: 0.8068\n",
      "Epoch 124/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3175 - acc: 0.8903 - val_loss: 0.5771 - val_acc: 0.8077\n",
      "Epoch 125/150\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3164 - acc: 0.8896 - val_loss: 0.5735 - val_acc: 0.8073\n",
      "Epoch 126/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3151 - acc: 0.8907 - val_loss: 0.5751 - val_acc: 0.8063\n",
      "Epoch 127/150\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3147 - acc: 0.8909 - val_loss: 0.5730 - val_acc: 0.8093\n",
      "Epoch 128/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3138 - acc: 0.8913 - val_loss: 0.5813 - val_acc: 0.8057\n",
      "Epoch 129/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3128 - acc: 0.8907 - val_loss: 0.5932 - val_acc: 0.7988\n",
      "Epoch 130/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3118 - acc: 0.8919 - val_loss: 0.5763 - val_acc: 0.8065\n",
      "Epoch 131/150\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3108 - acc: 0.8923 - val_loss: 0.5788 - val_acc: 0.8070\n",
      "Epoch 132/150\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3102 - acc: 0.8929 - val_loss: 0.5787 - val_acc: 0.8105\n",
      "Epoch 133/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3087 - acc: 0.8925 - val_loss: 0.5799 - val_acc: 0.8060\n",
      "Epoch 134/150\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3081 - acc: 0.8928 - val_loss: 0.5792 - val_acc: 0.8045\n",
      "Epoch 135/150\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3071 - acc: 0.8931 - val_loss: 0.5938 - val_acc: 0.8015\n",
      "Epoch 136/150\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 0.3062 - acc: 0.8939 - val_loss: 0.5878 - val_acc: 0.8057\n",
      "Epoch 137/150\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3053 - acc: 0.8947 - val_loss: 0.5825 - val_acc: 0.8065\n",
      "Epoch 138/150\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3045 - acc: 0.8945 - val_loss: 0.5903 - val_acc: 0.8000\n",
      "Epoch 139/150\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3034 - acc: 0.8957 - val_loss: 0.5838 - val_acc: 0.8058\n",
      "Epoch 140/150\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3025 - acc: 0.8955 - val_loss: 0.5891 - val_acc: 0.8023\n",
      "Epoch 141/150\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3015 - acc: 0.8964 - val_loss: 0.5869 - val_acc: 0.8055\n",
      "Epoch 142/150\n",
      "50000/50000 [==============================] - 3s 51us/step - loss: 0.3006 - acc: 0.8967 - val_loss: 0.5895 - val_acc: 0.8103\n",
      "Epoch 143/150\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.2995 - acc: 0.8969 - val_loss: 0.5981 - val_acc: 0.8052\n",
      "Epoch 144/150\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.2988 - acc: 0.8970 - val_loss: 0.5882 - val_acc: 0.8070\n",
      "Epoch 145/150\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 0.2976 - acc: 0.8981 - val_loss: 0.5905 - val_acc: 0.8042\n",
      "Epoch 146/150\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.2964 - acc: 0.8979 - val_loss: 0.5884 - val_acc: 0.8095\n",
      "Epoch 147/150\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.2962 - acc: 0.8983 - val_loss: 0.5922 - val_acc: 0.8035\n",
      "Epoch 148/150\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.2943 - acc: 0.8989 - val_loss: 0.5914 - val_acc: 0.8053\n",
      "Epoch 149/150\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.2936 - acc: 0.8993 - val_loss: 0.5917 - val_acc: 0.8068\n",
      "Epoch 150/150\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.2929 - acc: 0.8997 - val_loss: 0.5957 - val_acc: 0.8023\n"
     ]
    }
   ],
   "source": [
    "#  This cell may take several minutes to run\n",
    "random.seed(123)\n",
    "bigger_data_model = models.Sequential()\n",
    "bigger_data_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "bigger_data_model.add(layers.Dense(25, activation='relu'))\n",
    "bigger_data_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "bigger_data_model.compile(optimizer='SGD', \n",
    "                          loss='categorical_crossentropy', \n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "bigger_data_model_val = bigger_data_model.fit(X_train_tokens_bigger,  \n",
    "                                              y_train_lb_bigger,  \n",
    "                                              epochs=150,  \n",
    "                                              batch_size=256,  \n",
    "                                              validation_data=(X_val_tokens_bigger, y_val_lb_bigger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 5s 99us/step\n",
      "Training Loss: 0.287 \n",
      "Training Accuracy: 0.903\n",
      "----------\n",
      "4000/4000 [==============================] - 0s 112us/step\n",
      "Test Loss: 0.596 \n",
      "Test Accuracy: 0.802\n"
     ]
    }
   ],
   "source": [
    "results_train = bigger_data_model.evaluate(X_train_tokens_bigger, y_train_lb_bigger)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = bigger_data_model.evaluate(X_val_tokens_bigger, y_val_lb_bigger)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs and no regularization technique, you were able to get both better test accuracy and loss. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance! \n",
    "\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database \n",
    "\n",
    "\n",
    "## Summary  \n",
    "\n",
    "In this lesson, you built deep learning models using a validation set and used several techniques such as L2 and L1 regularization, dropout regularization, and early stopping to improve the accuracy of your models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
